{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Maia Rosengarten <br/>\n",
    "SID: 23572580<br/>\n",
    "March 27, 2017<br/>\n",
    "Kaggle Score:<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy as sp\n",
    "from scipy.stats import logistic as sig\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "from scipy import io\n",
    "import sklearn\n",
    "from sklearn.preprocessing import normalize\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NP FNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vec(arr): # d b 1 --> (d, 1)\n",
    "    return arr.reshape((arr.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def arr(vec): # 1 by d --> (d, )\n",
    "    return vec.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING FNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processCategoricalVars(lstIndicesOfCatFeatures):\n",
    "    '''\n",
    "        Replaces all categorical variables that take on n possible values with\n",
    "        n new columns in the design matrix. \n",
    "            e.g. If \"Weather\" takes on \"sunny\" or \"rainy\" would create two new features (columns)\n",
    "                in the design matrix, one 'sunny' and the other 'rainy'. If sample point's weather is\n",
    "                sunny, the new 'sunny' feature would take value 1, while 'rainy' would take 0.\n",
    "        Args:\n",
    "            lstIndicesOfCatFeatures (int lst) - list of indices in original design matrix where features are\n",
    "                categorical. Need to be updated.\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fillMissingValuesWithAverage(designMatrix, isCat)\n",
    "    '''\n",
    "        Iterates over every value of every feature and replaces '?' (missing) with\n",
    "        feature mean (if quantitative) and feature mode (if categorical)\n",
    "        \n",
    "        Args:\n",
    "            designMatrix (nxd ndarray) - training data w n sample points and d features\n",
    "            isSCat (bool) - boolean set to True if feature is categorical, False if quantitative\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fillMissingValuesWithKNN(designMatrix, k):\n",
    "    '''\n",
    "        Iterates over every value of every feature and replaces '?' (missing) with\n",
    "        value returned by K Nearest Neighbors\n",
    "        \n",
    "        Args:\n",
    "            designMatrix (nxd ndarray) - training data w n sample points and d features\n",
    "            k (int) - determines the number of neighbors that will determine point's value\n",
    "    \n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTIL FNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotCostFN(costs):\n",
    "    '''\n",
    "        Plots cost function as a function of number of iterations\n",
    "        Args:\n",
    "            costs (ndarray) - lst of costs per iteration of gradient descent\n",
    "    '''\n",
    "   \n",
    "    iters = [i for i in range(len(costs))]\n",
    "    plt.plot(iters, costs)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.title(\"Cost Per Iteration of Gradient Descent\")\n",
    "    plt.xlabel('numIterations')\n",
    "    plt.ylabel('cost')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateCsv(aryPredictions, strCsvName):\n",
    "    '''\n",
    "    Writes predictions of testSet to csv file\n",
    "    Args:\n",
    "        aryPredictions (ndarray) - (nx1)-array of predictions given size n test (or valid) set\n",
    "        strCsvName (str) - name of csv file to write to\n",
    "    '''\n",
    "    with open(strCsvName + '.csv', 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for i in range(len(aryPredictions)):\n",
    "            writer.writerow([i, aryPredictions[i]])\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readCSV(strFileName):\n",
    "    '''\n",
    "    Reads CSV file and returns data, labels tuple\n",
    "    Args:\n",
    "        strFileName (str) - path of csv file to read\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeCleanedFeaturesToCSV(cleanedDesignMatrix):\n",
    "    '''\n",
    "        Writes cleaned version of design matrix (filled in missing values, processed categorical variables)\n",
    "        to a CSV file to speed up work (don't need to preprocess every time you run code)\n",
    "        \n",
    "        Args: \n",
    "            cleanedDesignMatrix  \n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-dcd7a6c67270>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-dcd7a6c67270>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    def __init__:\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class DecisionTree(params):\n",
    "    def __init__:\n",
    "        #initialize with the params\n",
    "        self.root\n",
    "        self.attributeBag #<--ASK ABOUT THIS\n",
    "          \n",
    "    def impurity(left_label_hist, right_label_hist):\n",
    "        '''\n",
    "            A method that takes in the result of a split: two histograms\n",
    "            that count the frequencies of labels on the ”left” and ”right” side of that split.\n",
    "            Calculates and outputs a scalar value representing the impurity (”badness”) of the \n",
    "            specified split on the input data (CHECK LECTURE NOTES WHAT THIS IS)  \n",
    "            \n",
    "            Args:\n",
    "                left_label_hist (dict) - dictionary of label values to frequencies on the left side of the split\n",
    "                right_label_hist (dict) - dictionary of label values to frequencies on the right side of the split\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "    def chooseSplitRule(train_data, train_labels):\n",
    "        '''\n",
    "            Finds the best split rule for a Node using the impurity measure and input data. \n",
    "            Exhaustively tries threshold values from the data and chooses the combination of \n",
    "            split feature and threshold with the lowest impurity value. \n",
    "           \n",
    "            Args:\n",
    "               train_data (ndarray) - cleaned train data \n",
    "               train_labels (ndarray) - train labels\n",
    "            \n",
    "            Returns (bestSplitFeatIndex, bestSplitValue)\n",
    "            \n",
    "            \n",
    "            \n",
    "            If xi is quantitative, sort points in S by feature xi; \n",
    "            remove duplicates; [duplicate values, not points]\n",
    "            try splitting between each pair of consecutive values.\n",
    "            [We can radix sort the values in linear time, and if n is huge we should.]\n",
    "            \n",
    "            Clever Bit: As you scan sorted list from left to right, \n",
    "            you can update entropy in O(1) time per point! \n",
    "            [This is important for obtaining a fast tree-building time.] <--GO OVER THIS.\n",
    "            \n",
    "            Need to deal with case when a child is empty (skip that split)\n",
    "    \n",
    "        '''\n",
    "        \n",
    "        for featureIndex in range(len(train_data.shape[1])):\n",
    "            lstPossibleSplits = set(train_data[, featureIndex])\n",
    "            numSplitsToTry = len(lstPossibleSplits)\n",
    "        left_label_hist = dict((elem,0) for elem in lstPossibleSplits)\n",
    "        right_label_hist = dict((elem,0) for elem in lstPossibleSplits)\n",
    "        \n",
    "        #THIS IS WHAT I NEED HELP WITH --> CLARIFYING HOW TO MOVE FROM LEFT TO RIGHT AND CALCULATE THE HISTOGRAMS\n",
    "        \n",
    "    def computeEntropy(left_dict, right_dict):\n",
    "        '''\n",
    "            Given left and right sets compute the weighted average entropy.\n",
    "            \n",
    "            Args:\n",
    "                left_before, right_before, left_after, right_after\n",
    "            \n",
    "            Returns weighted average entropy from\n",
    "            \n",
    "            QUESTION: SHOULD ARGS BE DICTIONARIES OR LISTS OF CLASSES??\n",
    "        \n",
    "        '''\n",
    "        totalLeft = sum(left_dict.values()) #assumes inputs are dictionaries\n",
    "        totalRight = sum(right_dict.values()) \n",
    "        dictLeftLabelsToFreq = dict((label, 0) for label in totalLeft.keys())\n",
    "        dictRightLabelsToFreq = dict((label, 0) for label in totalRight.keys())\n",
    "        for label, count in left_dict.items():\n",
    "            dictLeftLabelsToFreq[label] = count/totalLeft\n",
    "        for label, count in right_dict.items():\n",
    "            dictRightLabelsToFreq[label] = count/totalRight \n",
    "        \n",
    "        leftEntropy = 0\n",
    "        for label in left_dict.keys():\n",
    "            prob = dictLeftLabelsToFreq[label]\n",
    "            leftEntropy -= prob * np.log(prob)\n",
    "            \n",
    "        rightEntropy = 0\n",
    "        for label in right_dict.keys():\n",
    "            prob = dictRightLabelsToFreq[label]\n",
    "            rightEntropy -= prob * np.log(prob)\n",
    "            \n",
    "        averageEntropy = (totalLeft * leftEntropy + totalRight * rightEntropy)/(totalLeft + totalRight)\n",
    "        return averageEntropy\n",
    "           \n",
    "        \n",
    "    def computeInfoGainFromSplit(left_before, right_before, left_after, right_after):\n",
    "        '''\n",
    "            Computes and returns the information gain from a split\n",
    "            \n",
    "            Args:\n",
    "                left_before (dict) - dictionary mapping labels to count in left set before split\n",
    "                right_before (dict) - dictionary mapping labels to count in right set before split\n",
    "                left_after (dict) - dictionary mapping labels to count in left set after split\n",
    "                right_after (dict) - dictionary mapping labels to count in right set after split\n",
    "        \n",
    "        ''' \n",
    "        entropyBeforeSplit = measureEntropy(left_before, right_before)\n",
    "        entropyAfterSplit = measureEntropy(left_after, right_after)\n",
    "        intInfoGain = entropyBeforeSplit - entropyAfterSplit\n",
    "        return intInfoGain\n",
    "        \n",
    "    \n",
    "    def train(train_data, train_labels, height_cap):\n",
    "        '''\n",
    "            Sets the root of the decision tree to the root node of the resulting tree from buildTree.\n",
    "            Args:\n",
    "                train_data (ndarray) - cleaned training data\n",
    "                train_lablels (ndarray) - cleaned training labels\n",
    "                height_cap (int) - prevents tree from growing too large\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        root = buildTree(train_data, train_labels, height_cap)\n",
    "        this.root = root \n",
    "    \n",
    "    def buildTree(train_data, train_labels, height_cap):\n",
    "        '''\n",
    "            Recursively grows a decision tree by constructing nodes. Using the impurity and segmenter methods, \n",
    "            attempts to find a configuration of nodes that best splits the input data. \n",
    "            This function figures out the split rules that each node should have and figures out \n",
    "            when to stop growing the tree and insert a leaf node.  \n",
    "            \n",
    "            This phase is training the data.\n",
    "            \n",
    "            Args:\n",
    "                train_data (ndarray) - n by d matrix of cleaned training data\n",
    "                train_labels (ndarray) - n by 1 vector of training labels\n",
    "                height_cap (int) - caps the height of the tree to prevent it from growing too large\n",
    "                \n",
    "        '''\n",
    "        \n",
    "        setSize = train_data.shape[0]\n",
    "        lstUniqueLabels = set(train_labels)\n",
    "        if len(lstUniqueLabels)==1:\n",
    "            label = lstUniqueLabels[0]\n",
    "            return Node(None, None, None, None, label)\n",
    "        else:\n",
    "            bestSplitFeatIndex, bestSplitVal = chooseSplitRuleAndValue(train_data, train_labels)\n",
    "            left = [] #maybe should be ditionaries?\n",
    "            right = [] #maybe should be dictionaries?\n",
    "            for sample in range(len(train_data.shape[0])):\n",
    "                if sample[bestSplitFeatIndex] <= bestSplitValue:\n",
    "                    left.append(sample)\n",
    "                else:\n",
    "                    right.append(sample)\n",
    "            return Node(bestSplitFeatIndex, bestSplitVal,  buildTree(left), buildTree(right))                     \n",
    "    \n",
    "    def predict(test_data):\n",
    "        '''\n",
    "            Given a data point, traverse the tree to find the best label to classify the data point. \n",
    "            Start at the root node and evaluate split rules as you traverse until you reach a leaf. \n",
    "            Choose that leaf nodes label as your output label.\n",
    "            \n",
    "            test_data (ndarray) - n by d matrix of points to classify. \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "    \n",
    "    class Node:\n",
    "        def init(self, featureIndex, splitVal, rightChild, leftChild, label):\n",
    "            self.splitFeatIndex = featureIndex \n",
    "            self.splitValue = splitVal\n",
    "            self.left = leftChild\n",
    "            self.right = rightChild \n",
    "            self.label = label #if set, this node is a leaf (mode), else is intermediary\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomForest(params)\n",
    "    def __init__:\n",
    "        self.attributeBag #subset of features to train each node on in different trees\n",
    "        self.dataBag #subset of data to train each tree on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'training_labels', 'test_data', 'training_data'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SPAM DATA\n",
    "\n",
    "spamData = sp.io.loadmat(\"dist/spam_data.mat\")\n",
    "spamData.keys()\n",
    "spamTrainX = spamData['training_data']\n",
    "spamTrainY = spamData['training_labels']\n",
    "spamTest = spamData['test_data']\n",
    "\n",
    "spamTest = sklearn.preprocessing.normalize(spamTest.astype(float))\n",
    "spamTrainX = sklearn.preprocessing.normalize(spamTrainX.astype(float))\n",
    "\n",
    "combined = np.hstack([spamTrainX, spamTrainY])\n",
    "np.random.shuffle(combined)\n",
    "spamTrainX = combined[:, :-1]\n",
    "spamTrainY = combined[:, -1]\n",
    "\n",
    "#maybe add a bias feature\n",
    "\n",
    "trainX, validX, trainY, validY = train_test_split(trainX, trainY, test_size=.1, random_state=42)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [189hw]",
   "language": "python",
   "name": "Python [189hw]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
