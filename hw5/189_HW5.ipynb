{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Maia Rosengarten <br/>\n",
    "SID: 23572580<br/>\n",
    "March 27, 2017<br/>\n",
    "Kaggle Score:<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy as sp\n",
    "from scipy.stats import logistic as sig\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "from scipy import io\n",
    "import sklearn\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction import DictVectorizer as dv\n",
    "from sklearn.preprocessing import Imputer as imp\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NP FNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vec(arr): # d b 1 --> (d, 1)\n",
    "    return arr.reshape((arr.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def arr(vec): # 1 by d --> (d, )\n",
    "    return vec.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING FNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorizeMatrix(data):\n",
    "    '''\n",
    "        Replaces all categorical variables that take on n possible values with\n",
    "        n new columns in the design matrix. \n",
    "            e.g. If \"Weather\" takes on \"sunny\" or \"rainy\" would create two new features (columns)\n",
    "                in the design matrix, one 'sunny' and the other 'rainy'. If sample point's weather is\n",
    "                sunny, the new 'sunny' feature would take value 1, while 'rainy' would take 0.\n",
    "        Args:\n",
    "            data - a dataframe, the design matrix\n",
    "    \n",
    "    '''\n",
    "    v = DictVectorizer(sparse=False)\n",
    "    data = v.fit_transform(data.T.to_dict().values())\n",
    "    return data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fillMissingValuesWithAverage(designMatrix, isCat):\n",
    "    '''\n",
    "        Iterates over every value of every feature and replaces '?' (missing) with\n",
    "        feature mean (if quantitative) and feature mode (if categorical)\n",
    "        \n",
    "        Args:\n",
    "            designMatrix (nxd ndarray) - training data w n sample points and d features\n",
    "            isSCat (bool) - boolean set to True if feature is categorical, False if quantitative\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fillMissingValuesWithKNN(designMatrix, k):\n",
    "    '''\n",
    "        Iterates over every value of every feature and replaces '?' (missing) with\n",
    "        value returned by K Nearest Neighbors\n",
    "        \n",
    "        Args:\n",
    "            designMatrix (nxd ndarray) - training data w n sample points and d features\n",
    "            k (int) - determines the number of neighbors that will determine point's value\n",
    "    \n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTIL FNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotCostFN(costs):\n",
    "    '''\n",
    "        Plots cost function as a function of number of iterations\n",
    "        Args:\n",
    "            costs (ndarray) - lst of costs per iteration of gradient descent\n",
    "    '''\n",
    "   \n",
    "    iters = [i for i in range(len(costs))]\n",
    "    plt.plot(iters, costs)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.title(\"Cost Per Iteration of Gradient Descent\")\n",
    "    plt.xlabel('numIterations')\n",
    "    plt.ylabel('cost')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateCsv(aryPredictions, strCsvName):\n",
    "    '''\n",
    "    Writes predictions of testSet to csv file\n",
    "    Args:\n",
    "        aryPredictions (ndarray) - (nx1)-array of predictions given size n test (or valid) set\n",
    "        strCsvName (str) - name of csv file to write to\n",
    "    '''\n",
    "    with open(strCsvName + '.csv', 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for i in range(len(aryPredictions)):\n",
    "            writer.writerow([i, aryPredictions[i]])\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readCSV(strFileName):\n",
    "    '''\n",
    "    Reads CSV file and returns data, labels tuple\n",
    "    Args:\n",
    "        strFileName (str) - path of csv file to read\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeCleanedFeaturesToCSV(cleanedDesignMatrix):\n",
    "    '''\n",
    "        Writes cleaned version of design matrix (filled in missing values, processed categorical variables)\n",
    "        to a CSV file to speed up work (don't need to preprocess every time you run code)\n",
    "        \n",
    "        Args: \n",
    "            cleanedDesignMatrix  \n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, root=None, max_depth=50, leaf_condition=1):\n",
    "        self.root = root\n",
    "        self.max_depth = max_depth\n",
    "        self.leaf_condition = leaf_condition\n",
    "    \n",
    "    def train(self, train_data, train_labels):\n",
    "        '''\n",
    "            Sets the root of the decision tree to the root node of the resulting tree from buildTree.\n",
    "            Args:\n",
    "                train_data (ndarray) - cleaned training data\n",
    "                train_lablels (ndarray) - cleaned training labels\n",
    "                height_cap (int) - prevents tree from growing too large\n",
    "        \n",
    "        '''\n",
    "        node_depth = 0\n",
    "        self.root = self.buildTree(train_data, train_labels, self.max_depth, node_depth, self.leaf_condition)\n",
    "    \n",
    "    def buildTree(self, train_data, train_labels, max_depth, node_depth, leaf_condition=1):\n",
    "        '''\n",
    "            Recursively grows a decision tree by constructing nodes. Using the impurity and segmenter methods, \n",
    "            attempts to find a configuration of nodes that best splits the input data. \n",
    "            This function figures out the split rules that each node should have and figures out \n",
    "            when to stop growing the tree and insert a leaf node.  \n",
    "            \n",
    "            This phase is training the data.\n",
    "            \n",
    "            Args:\n",
    "                train_data (ndarray) - n by d matrix of cleaned training data\n",
    "                train_labels (ndarray) - n by 1 vector of training labels\n",
    "                max_depth (int) - caps the height of the tree to prevent it from growing too large\n",
    "                \n",
    "        '''\n",
    "        #fix this stopping condition\n",
    "        numSamples = train_data.shape[0]\n",
    "        numClass1 = np.sum(train_labels)\n",
    "        numClass0 = numSamples - numClass1\n",
    "        \n",
    "#         #is leaf\n",
    "        isLeaf, label = self.isMajClassLeaf(numSamples, numClass1, numClass0, leaf_condition)\n",
    "        if (isLeaf):\n",
    "            return Node(label=label)\n",
    "        elif (max_depth==0): #isLeaf\n",
    "            if (numClass1 > numClass0):\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "            return Node(label=label)  \n",
    "        else:\n",
    "            node = Node()\n",
    "            node.chooseSplitRule(train_data, train_labels)\n",
    "            \n",
    "            leftIndices = [] \n",
    "            rightIndices = [] \n",
    "#             lstSortedTupIndex2FeatVal = [i for i in sorted(enumerate(train_data[:, node.splitFeatIndex]), key=lambda x: x[1])]\n",
    "            \n",
    "#             lstSortedIndices = [i[0] for i in lstSortedTupIndex2FeatVal]\n",
    "#             lstSortedFeatVals = [i[1] for i in lstSortedTupIndex2FeatVal]\n",
    "\n",
    "            for i in range(train_data.shape[0]):\n",
    "                if (train_data[i, node.splitFeatIndex] <= node.splitVal):\n",
    "                    leftIndices.append(i)\n",
    "                else:\n",
    "                    rightIndices.append(i)\n",
    "            \n",
    "            leftSetX = train_data[leftIndices, :]\n",
    "            leftSetY = train_labels[leftIndices]\n",
    "\n",
    "            \n",
    "            rightSetX = train_data[rightIndices, :]\n",
    "            rightSetY= train_labels[rightIndices]\n",
    "\n",
    "            sizeRightSet = len(rightSetX)\n",
    "            sizeLeftSet = len(leftSetX)\n",
    "            \n",
    "            if (sizeRightSet==0):\n",
    "                if (np.sum(rightSetY) > sizeRightSet/2):\n",
    "                    return Node(label=1)\n",
    "                return Node(label=0)\n",
    "            \n",
    "            if (sizeLeftSet==0):\n",
    "                if (np.sum(leftSetY)> sizeLeftSet/2):\n",
    "                    return Node(label=1)\n",
    "                return Node(label=0)\n",
    "                \n",
    "            max_depth -=1\n",
    "            node_depth+=1\n",
    "            return Node(featureIndex=node.splitFeatIndex, splitVal=node.splitVal, leftChild=self.buildTree(leftSetX, leftSetY, max_depth, node_depth, leaf_condition), rightChild=self.buildTree(rightSetX, rightSetY, max_depth, node_depth, leaf_condition), depth=node_depth)\n",
    "    \n",
    "        \n",
    "    def predictPoint(self, node, data_point):\n",
    "        '''\n",
    "            Given a data point, traverse the tree to find the best label to classify the data point. \n",
    "            Start at the root node and evaluate split rules as you traverse until you reach a leaf. \n",
    "            Choose that leaf nodes label as your output label.\n",
    "            \n",
    "            data_point (ndarray) - (dx1) single point to classify with d features \n",
    "        \n",
    "        '''\n",
    "        if (node.label is not None):\n",
    "            tortn = node.label\n",
    "            return tortn  \n",
    "        if (data_point[node.splitFeatIndex] <= node.splitVal):\n",
    "            return self.predictPoint(node.left, data_point)\n",
    "        else:\n",
    "            return self.predictPoint(node.right, data_point)\n",
    "    \n",
    "    def predict(self, dataMatrix):\n",
    "        '''\n",
    "            Given a matrix of test samples, predict labels.\n",
    "            \n",
    "            dataMatrix (ndarray) - (nxd) unseen samples with d dimensions \n",
    "        ''' \n",
    "        predictions = []\n",
    "        for i in range(dataMatrix.shape[0]):   \n",
    "            label = self.predictPoint(self.root, dataMatrix[i])\n",
    "            predictions.append(label)\n",
    "        return predictions\n",
    "    \n",
    "    def isMajClassLeaf(self, numSamples, numClass1, numClass0, threshold):\n",
    "        '''\n",
    "            Returns (True, y) if threshold% of points in a node are of a particular class\n",
    "            where y is the label of the leaf.\n",
    "\n",
    "        '''     \n",
    "        if (numClass1 >= (threshold * numSamples)):\n",
    "            return True, 1\n",
    "        elif (numClass0 >= (threshold * numSamples)):\n",
    "            return True, 0\n",
    "        return False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def isPureLeaf(numSamples, numClass1, numClass0):\n",
    "#     '''\n",
    "#         Returns (True, y) if leaf is pure where y is the label of the leaf.\n",
    "    \n",
    "#     '''\n",
    "#     if (numClass1 == numSamples): \n",
    "#         return True, 1\n",
    "#     elif (numClass0 == numSamples):\n",
    "#         return True, 0\n",
    "#     return False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, numTrees, train_data, train_labels, leaf_condition):\n",
    "#         self.attributeBag = attributeBag #subset of features to train each node on in different trees\n",
    "#         self.dataBag = dataBag #subset of data to train each tree on \n",
    "        self.train_data = train_data\n",
    "        self.train_labels = train_labels\n",
    "        self.trees = []\n",
    "        self.predictions = []\n",
    "        self.numTrees = numTrees\n",
    "        self.leaf_condition = leaf_condition\n",
    "        for i in range(self.numTrees):\n",
    "            self.trees.append(DecisionTree(leaf_condition=self.leaf_condition))          \n",
    "                     \n",
    "    def train(self, train_data):\n",
    "        i = 0\n",
    "        for tree in self.trees:\n",
    "            print('tree ' + str(i))\n",
    "            numSamples = self.train_data.shape[0]\n",
    "            random = np.random.choice(numSamples, numSamples, replace=True)\n",
    "            subset_data = self.train_data[random]\n",
    "            subset_labels = self.train_labels[random]\n",
    "            tree.train(subset_data, subset_labels)\n",
    "            i+=1\n",
    "    \n",
    "    def predict(self, valid_data):\n",
    "        predictions = []\n",
    "        for point in valid_data:\n",
    "            labels = []\n",
    "            for tree in self.trees:\n",
    "                labels.append(tree.predictPoint(tree.root, point))\n",
    "            if (np.sum(labels) >= 0.5 * self.numTrees):\n",
    "                predictions.append(1)\n",
    "            else:\n",
    "                predictions.append(0)\n",
    "        self.predictions = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through all the spam data\n",
    "Bag of words: have b words in vocabulary and vector is in R^b. vi = 1 if word i appears in document\n",
    "if vocabulary is \"ham\", \"spam\", \"sam\" then you have your sentences: \"sam likes spam\": [0, 1, 1]...\n",
    "\n",
    "When taking mutually exclusive sets, can overfit. Writing a better featurize.py\n",
    "Take most commonly occurring words and make it an exclusive set. I get set of all words minus the stuff that the \n",
    "two classes have in common (like \"the\"). vocab of one class, vocab of two class. and can do set(v1) - set(v2) (but can overfit)...\n",
    "\n",
    "if that can overfit, then can add other words from in between randomly. Or dimensionality reduction on total bag of words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, featureIndex=0, splitVal=None, leftChild=None, rightChild=None, label=None, depth=1):\n",
    "        self.splitFeatIndex = featureIndex \n",
    "        self.splitVal = splitVal\n",
    "        self.left = leftChild\n",
    "        self.right = rightChild \n",
    "        self.label = label \n",
    "        self.depth = depth\n",
    "    \n",
    "    \n",
    "    def chooseSplitRule(self, train_data, train_labels):\n",
    "        '''\n",
    "            Finds the best split rule for a Node using the impurity measure and input data. \n",
    "            Exhaustively tries threshold values from the data and chooses the combination of \n",
    "            split feature and threshold with the lowest impurity value. \n",
    "           \n",
    "            Args:\n",
    "               train_data (ndarray) - cleaned train data \n",
    "               train_labels (ndarray) - train labels\n",
    "            \n",
    "            Returns (bestSplitFeatIndex, bestSplitValue)\n",
    "                 \n",
    "            \n",
    "            If xi is quantitative, sort points in S by feature xi; \n",
    "            remove duplicates; [duplicate values, not points]\n",
    "            try splitting between each pair of consecutive values.\n",
    "            [We can radix sort the values in linear time, and if n is huge we should.]\n",
    "            \n",
    "            Clever Bit: As you scan sorted list from left to right, \n",
    "            you can update entropy in O(1) time per point! \n",
    "            [This is important for obtaining a fast tree-building time.] <--GO OVER THIS.\n",
    "            \n",
    "            Need to deal with case when a child is empty (skip that split)\n",
    "    \n",
    "        '''\n",
    "        #<= split value --> left, > split value --> right\n",
    "        \n",
    "        #H before (PARENT NODE)  \n",
    "        currNodeSize = len(train_data)\n",
    "        currClass1Count = sum(train_labels) \n",
    "        currClass0Count = currNodeSize - currClass1Count\n",
    "        currNodeEntropy = self.computeEntropy((currNodeSize, currClass0Count, currClass1Count))\n",
    "        bestInfoGain = -float(\"inf\")\n",
    " \n",
    "        #SET UP FOR SPLIT\n",
    "        labelDictRight = {0: currClass0Count, 1: currClass1Count}\n",
    "        labelDictLeft = {0:0, 1:0}\n",
    "        sizeRightSet = currNodeSize\n",
    "        sizeLeftSet = 0\n",
    "        \n",
    "        for featureIndex in range(train_data.shape[1]):\n",
    "            sortedIndices = np.argsort(train_data[:, featureIndex])\n",
    "            sortedVals = np.sort(train_data[:, featureIndex])\n",
    "            \n",
    "            lstUniqueVal, lstUniqueCounts = np.unique(sortedVals, return_counts=True)\n",
    "            runningIndex = - 1\n",
    "            for i in range(len(lstUniqueVal)):\n",
    "                runningIndex += (lstUniqueCounts[i])\n",
    "                tempSplitVal = lstUniqueVal[i]\n",
    "                \n",
    "                sizeLeftSet = runningIndex + 1\n",
    "                sizeRightSet = currNodeSize - sizeLeftSet\n",
    "                \n",
    "                labelDictLeft[1] = np.sum(train_labels[sortedIndices[:runningIndex+1]])\n",
    "                labelDictRight[1] = np.sum(train_labels[sortedIndices[runningIndex+1:]])\n",
    "                labelDictRight[0] = sizeRightSet - labelDictRight[1]\n",
    "                labelDictLeft[0] = sizeLeftSet - labelDictLeft[1]\n",
    "                \n",
    "                tupSizeLeftClassCounts = (sizeLeftSet, labelDictLeft[0], labelDictLeft[1])\n",
    "                tupSizeRightClassCounts = (sizeRightSet, labelDictRight[0], labelDictRight[1])\n",
    "                entropyAfterSplit = self.entropyAfterSplit(tupSizeLeftClassCounts, tupSizeRightClassCounts) \n",
    "                infoGain = currNodeEntropy - entropyAfterSplit\n",
    "                \n",
    "                if infoGain > bestInfoGain:\n",
    "                    bestInfoGain = infoGain\n",
    "                    bestSplitVal = tempSplitVal\n",
    "                    bestSplitIndex = featureIndex \n",
    "  \n",
    "        self.splitFeatIndex = bestSplitIndex \n",
    "        self.splitVal = bestSplitVal\n",
    "              \n",
    "    def entropyAfterSplit(self, tupSizeLeftClassCounts, tupSizeRightClassCounts):\n",
    "        '''\n",
    "            A method that takes in the result of a split: two histograms\n",
    "            that count the frequencies of labels on the ”left” and ”right” side of that split.\n",
    "            Calculates and outputs a scalar value representing the impurity (”badness”) of the \n",
    "            specified split on the input data. In lecture, this was refered to as H_after where H\n",
    "            is the function that computes entropy.\n",
    "            \n",
    "            Args:\n",
    "                tupSizeLeftClassCounts (tup) - \n",
    "                tupSizeRightClassCounts (tup) - \n",
    "        \n",
    "        '''\n",
    "        sizeLeftSet = tupSizeLeftClassCounts[0]\n",
    "        sizeRightSet = tupSizeRightClassCounts[0]\n",
    "\n",
    "        entropyLeft = self.computeEntropy(tupSizeLeftClassCounts)\n",
    "        entropyRight = self.computeEntropy(tupSizeRightClassCounts)\n",
    "        entropyAfterSplit = (entropyLeft * sizeLeftSet + entropyRight * sizeRightSet)/ (sizeLeftSet + sizeRightSet)\n",
    "        return entropyAfterSplit\n",
    "    \n",
    "    \n",
    "    def computeEntropy(self, tupleSizeSetClassCounts):\n",
    "        '''\n",
    "            Compute the entropy of a set (assume 2 classes). \n",
    "            \n",
    "            Args:\n",
    "                tupleSetSizeClassSize (tup) - (cardinality of set, cardinality of class 0 in set) \n",
    "        '''\n",
    "        sizeSet, countClass0, countClass1 = tupleSizeSetClassCounts\n",
    "        if (sizeSet==0):\n",
    "            return 0\n",
    "        probClass0 = countClass0/sizeSet\n",
    "        if (probClass0 == 1 or probClass0 == 0):\n",
    "            return 0\n",
    "        else: \n",
    "            entropy = (- probClass0 * np.log2(probClass0)) - ((1 - probClass0) * np.log2(1 - probClass0))\n",
    "        return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#SPAM DATA\n",
    "\n",
    "spamData = sp.io.loadmat(\"dist/spam_data.mat\")\n",
    "spamData.keys()\n",
    "spamTrainX = spamData['training_data']\n",
    "spamTrainY = spamData['training_labels']\n",
    "spamTest = spamData['test_data']\n",
    "\n",
    "combined = np.hstack([spamTrainX, spamTrainY.T])\n",
    "np.random.shuffle(combined)\n",
    "combined = imp().fit_transform(combined)\n",
    "\n",
    "spamTrainX = combined[:, :-1]\n",
    "spamTrainY = combined[:, -1]\n",
    "\n",
    "#maybe add a bias feature\n",
    "trainX, validX, trainY, validY = train_test_split(spamTrainX, spamTrainY, test_size=.1, random_state=42)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-402-8cc603ebdc03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdecisionTree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaf_condition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.84\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdecisionTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-387-82dd80f7de71>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_data, train_labels)\u001b[0m\n\u001b[1;32m     15\u001b[0m         '''\n\u001b[1;32m     16\u001b[0m         \u001b[0mnode_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuildTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaf_condition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuildTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaf_condition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-387-82dd80f7de71>\u001b[0m in \u001b[0;36mbuildTree\u001b[0;34m(self, train_data, train_labels, max_depth, node_depth, leaf_condition)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchooseSplitRule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mleftIndices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-239-7f64c7fc8652>\u001b[0m in \u001b[0;36mchooseSplitRule\u001b[0;34m(self, train_data, train_labels)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0msizeRightSet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrNodeSize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msizeLeftSet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mlabelDictLeft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msortedIndices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrunningIndex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mlabelDictRight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msortedIndices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrunningIndex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mlabelDictRight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msizeRightSet\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlabelDictRight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "decisionTree = DecisionTree(max_depth=50, leaf_condition=.84)\n",
    "decisionTree.train(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = decisionTree.predict(validX)\n",
    "accuracy = accuracy_score(validY, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78026149304091097"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree 0\n",
      "tree 1\n",
      "tree 2\n",
      "tree 3\n",
      "tree 4\n",
      "tree 5\n",
      "tree 6\n",
      "tree 7\n",
      "tree 8\n",
      "tree 9\n",
      "tree 10\n",
      "tree 11\n",
      "tree 12\n",
      "tree 13\n",
      "tree 14\n",
      "tree 15\n",
      "tree 16\n",
      "tree 17\n",
      "tree 18\n",
      "tree 19\n",
      "tree 20\n",
      "tree 21\n",
      "tree 22\n",
      "tree 23\n",
      "tree 24\n",
      "tree 25\n",
      "tree 26\n",
      "tree 27\n",
      "tree 28\n",
      "tree 29\n",
      "tree 30\n",
      "tree 31\n",
      "tree 32\n",
      "tree 33\n",
      "tree 34\n",
      "tree 35\n",
      "tree 36\n",
      "tree 37\n",
      "tree 38\n",
      "tree 39\n",
      "tree 40\n",
      "tree 41\n",
      "tree 42\n",
      "tree 43\n",
      "tree 44\n",
      "tree 45\n",
      "tree 46\n",
      "tree 47\n",
      "tree 48\n",
      "tree 49\n"
     ]
    }
   ],
   "source": [
    "num_trees = 50\n",
    "leaf_condition = 0.84\n",
    "randForest = RandomForest(num_trees, trainX, trainY, leaf_condition)\n",
    "randForest.train(trainX)\n",
    "predictions = randForest.predict(validX)\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randForest.predictions is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(validY, randForest.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77815267819485445"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [189hw]",
   "language": "python",
   "name": "Python [189hw]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
