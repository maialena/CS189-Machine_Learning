{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Maia Rosengarten <br/>\n",
    "SID: 23572580<br/>\n",
    "March 27, 2017<br/>\n",
    "Kaggle Score:<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy as sp\n",
    "from scipy.stats import logistic as sig\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "from scipy import io\n",
    "import sklearn\n",
    "from sklearn.preprocessing import normalize\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NP FNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vec(arr): # d b 1 --> (d, 1)\n",
    "    return arr.reshape((arr.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def arr(vec): # 1 by d --> (d, )\n",
    "    return vec.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING FNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processCategoricalVars(lstIndicesOfCatFeatures):\n",
    "    '''\n",
    "        Replaces all categorical variables that take on n possible values with\n",
    "        n new columns in the design matrix. \n",
    "            e.g. If \"Weather\" takes on \"sunny\" or \"rainy\" would create two new features (columns)\n",
    "                in the design matrix, one 'sunny' and the other 'rainy'. If sample point's weather is\n",
    "                sunny, the new 'sunny' feature would take value 1, while 'rainy' would take 0.\n",
    "        Args:\n",
    "            lstIndicesOfCatFeatures (int lst) - list of indices in original design matrix where features are\n",
    "                categorical. Need to be updated.\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fillMissingValuesWithAverage(designMatrix, isCat)\n",
    "    '''\n",
    "        Iterates over every value of every feature and replaces '?' (missing) with\n",
    "        feature mean (if quantitative) and feature mode (if categorical)\n",
    "        \n",
    "        Args:\n",
    "            designMatrix (nxd ndarray) - training data w n sample points and d features\n",
    "            isSCat (bool) - boolean set to True if feature is categorical, False if quantitative\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fillMissingValuesWithKNN(designMatrix, k):\n",
    "    '''\n",
    "        Iterates over every value of every feature and replaces '?' (missing) with\n",
    "        value returned by K Nearest Neighbors\n",
    "        \n",
    "        Args:\n",
    "            designMatrix (nxd ndarray) - training data w n sample points and d features\n",
    "            k (int) - determines the number of neighbors that will determine point's value\n",
    "    \n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTIL FNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotCostFN(costs):\n",
    "    '''\n",
    "        Plots cost function as a function of number of iterations\n",
    "        Args:\n",
    "            costs (ndarray) - lst of costs per iteration of gradient descent\n",
    "    '''\n",
    "   \n",
    "    iters = [i for i in range(len(costs))]\n",
    "    plt.plot(iters, costs)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.title(\"Cost Per Iteration of Gradient Descent\")\n",
    "    plt.xlabel('numIterations')\n",
    "    plt.ylabel('cost')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateCsv(aryPredictions, strCsvName):\n",
    "    '''\n",
    "    Writes predictions of testSet to csv file\n",
    "    Args:\n",
    "        aryPredictions (ndarray) - (nx1)-array of predictions given size n test (or valid) set\n",
    "        strCsvName (str) - name of csv file to write to\n",
    "    '''\n",
    "    with open(strCsvName + '.csv', 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for i in range(len(aryPredictions)):\n",
    "            writer.writerow([i, aryPredictions[i]])\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readCSV(strFileName):\n",
    "    '''\n",
    "    Reads CSV file and returns data, labels tuple\n",
    "    Args:\n",
    "        strFileName (str) - path of csv file to read\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeCleanedFeaturesToCSV(cleanedDesignMatrix):\n",
    "    '''\n",
    "        Writes cleaned version of design matrix (filled in missing values, processed categorical variables)\n",
    "        to a CSV file to speed up work (don't need to preprocess every time you run code)\n",
    "        \n",
    "        Args: \n",
    "            cleanedDesignMatrix  \n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-dcd7a6c67270>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-dcd7a6c67270>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    def __init__:\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class DecisionTree(params):\n",
    "    def __init__:\n",
    "        self.root\n",
    "#         self.attributeBag #<--ASK ABOUT THIS\n",
    "          \n",
    "    def entropyAfterSplit(tupleSizeLeftSetClass0Count, tupleSizeRightSetClass0Count):\n",
    "        '''\n",
    "            A method that takes in the result of a split: two histograms\n",
    "            that count the frequencies of labels on the ”left” and ”right” side of that split.\n",
    "            Calculates and outputs a scalar value representing the impurity (”badness”) of the \n",
    "            specified split on the input data. In lecture, this was refered to as H_after where H\n",
    "            is the function that computes entropy.\n",
    "            \n",
    "            Args:\n",
    "                left_label_hist (dict) - dictionary of label values to frequencies on the left side of the split\n",
    "                right_label_hist (dict) - dictionary of label values to frequencies on the right side of the split\n",
    "        \n",
    "        \n",
    "        impurity = calculate entropy  - impurity is within a certain node; entropy is accross nodes\n",
    "        - purity: the state of something. \n",
    "        \n",
    "        Class A or Not A (binary)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        sizeLeftSet = tupleSizeLeftSetClass0Count[0]\n",
    "        sizeRightSet = tupleSizeRightSetClass0Count[0]\n",
    "        entropyLeft = computeEntropy(tupleSizeLeftSetClass0Count)\n",
    "        entropyRight = computeEntropy(tupleSizeRightSetClass0Count)\n",
    "        entropyAfterSplit = (entropyLeft * sizeLeftSet + entropyRight * sizeRightSet)/ (sizeLeftSet + sizeRightSet)\n",
    "        print(\"entropyAfterSplit: \" + str(entropyAfterSplit))\n",
    "        return (entropyAfterSplit)    \n",
    "    \n",
    "    \n",
    "    def computeEntropy(tupleSizeSetClass0Count):\n",
    "        '''\n",
    "            Compute the entropy of a set given two classes. \n",
    "            \n",
    "            Args:\n",
    "                tupleSetSizeClassSize (tup) - (cardinality of set, cardinality of class 0 in set) \n",
    "        \n",
    "            \n",
    "            QUESTION: SHOULD NODE STORE ITS ENTROPY??\n",
    "        \n",
    "        '''\n",
    "        sizeSet, countClass0 = tupleSizeSetClass0Count\n",
    "        probClass = countClass0/sizeSet\n",
    "        entropy = (- probClass * math.log(probClass, 2)) - ((1 - probClass) * math.log(1 - probClass, 2))\n",
    "        print('computeEntropy: ' + str(entropy))\n",
    "        return entropy\n",
    "           \n",
    "    \n",
    "    def chooseSplitRule(train_data, train_labels, currentNode):\n",
    "        '''\n",
    "            Finds the best split rule for a Node using the impurity measure and input data. \n",
    "            Exhaustively tries threshold values from the data and chooses the combination of \n",
    "            split feature and threshold with the lowest impurity value. \n",
    "           \n",
    "            Args:\n",
    "               train_data (ndarray) - cleaned train data \n",
    "               train_labels (ndarray) - train labels\n",
    "            \n",
    "            Returns (bestSplitFeatIndex, bestSplitValue)\n",
    "            \n",
    "            \n",
    "            \n",
    "            If xi is quantitative, sort points in S by feature xi; \n",
    "            remove duplicates; [duplicate values, not points]\n",
    "            try splitting between each pair of consecutive values.\n",
    "            [We can radix sort the values in linear time, and if n is huge we should.]\n",
    "            \n",
    "            Clever Bit: As you scan sorted list from left to right, \n",
    "            you can update entropy in O(1) time per point! \n",
    "            [This is important for obtaining a fast tree-building time.] <--GO OVER THIS.\n",
    "            \n",
    "            Need to deal with case when a child is empty (skip that split)\n",
    "    \n",
    "        '''\n",
    "        \n",
    "        #H before\n",
    "        #<= split value --> left, > split value --> right\n",
    "        currNodeFeatureSet = train_data[currentNode.splitFeatIndex]\n",
    "        currNodeClass0Count = len([i for i in currNodeFeatureSet if i < currentNode.splitValue]) #not sure if this is <, <=, > , >=\n",
    "        print(\"currNodeClas0Count: \" + str(currNodeClass0Count))\n",
    "        currNodeEntropy = computeEntropy((len(currNodeFeatureSet), currNodeClass0Count))\n",
    "        \n",
    "        sortedIndices = sort(currNodeFeatureSet)\n",
    "        bestSplitIndex = 0\n",
    "        bestSplitValue = 0\n",
    "        bestSplitIndex = 0\n",
    "        bestSplitVal = 0\n",
    "        bestinfoGain = -float(\"inf\")\n",
    "        bestleftIndices = []\n",
    "        bestrightIndices = []\n",
    "           \n",
    "        for featureIndex in range(len(train_data.shape[1])): #iterate over features\n",
    "            leftIndices = []\n",
    "            rightIndices = []\n",
    "            for sample in range(len(train_data.shape[0])):\n",
    "                splitValue = sample[featureIndex]\n",
    "                \n",
    "               \n",
    "\n",
    "#             lstPossibleSplits = set(train_data[, featureIndex])\n",
    "        \n",
    "            \n",
    "            leftCounts = (,,) #this keeps count of the elements left of split point\n",
    "            rightCounts = (,,) #this keeps count of the elements right of split piont\n",
    "                \n",
    "        '''\n",
    "        \n",
    "        1. Only have indices of the sample points, don't throw the sample points around.\n",
    "        2. lst holding the left vs the right points\n",
    "        3. Don't need to iterate through the lists to figure out if class 1 or 0...do that once when initialiez\n",
    "            - because sorting on index, cardinality of \n",
    "        4. entropy: compute entropy separately: all elements\n",
    "        \n",
    "        \n",
    "        bestEntropySoFar = thisEntorpy\n",
    "        bestFeatureSoFar = thisFeature\n",
    "        bestSplitValSoFar = thisSplitVal\n",
    "        bestLeftIndices = this.left_Indices\n",
    "        bestRightIndices = this.rightIndices\n",
    "        \n",
    "        '''\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    def computeInfoGainFromSplit(left_before, right_before, left_after, right_after):\n",
    "        '''\n",
    "            Computes and returns the information gain from a split\n",
    "            \n",
    "            Args:\n",
    "                left_before (dict) - dictionary mapping labels to count in left set before split\n",
    "                right_before (dict) - dictionary mapping labels to count in right set before split\n",
    "                left_after (dict) - dictionary mapping labels to count in left set after split\n",
    "                right_after (dict) - dictionary mapping labels to count in right set after split\n",
    "        \n",
    "        ''' \n",
    "        entropyBeforeSplit = measureEntropy(left_before, right_before)\n",
    "        entropyAfterSplit = measureEntropy(left_after, right_after)\n",
    "        intInfoGain = entropyBeforeSplit - entropyAfterSplit\n",
    "        return intInfoGain\n",
    "        \n",
    "    \n",
    "    def train(train_data, train_labels, height_cap):\n",
    "        '''\n",
    "            Sets the root of the decision tree to the root node of the resulting tree from buildTree.\n",
    "            Args:\n",
    "                train_data (ndarray) - cleaned training data\n",
    "                train_lablels (ndarray) - cleaned training labels\n",
    "                height_cap (int) - prevents tree from growing too large\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        root = buildTree(train_data, train_labels, height_cap)\n",
    "        this.root = root \n",
    "    \n",
    "    def buildTree(train_data, train_labels, height_cap):\n",
    "        '''\n",
    "            Recursively grows a decision tree by constructing nodes. Using the impurity and segmenter methods, \n",
    "            attempts to find a configuration of nodes that best splits the input data. \n",
    "            This function figures out the split rules that each node should have and figures out \n",
    "            when to stop growing the tree and insert a leaf node.  \n",
    "            \n",
    "            This phase is training the data.\n",
    "            \n",
    "            Args:\n",
    "                train_data (ndarray) - n by d matrix of cleaned training data\n",
    "                train_labels (ndarray) - n by 1 vector of training labels\n",
    "                height_cap (int) - caps the height of the tree to prevent it from growing too large\n",
    "                \n",
    "        '''\n",
    "        \n",
    "        setSize = train_data.shape[0]\n",
    "        lstUniqueLabels = set(train_labels)\n",
    "        if len(lstUniqueLabels)==1:\n",
    "            label = lstUniqueLabels[0]\n",
    "            return Node(None, None, None, None, label)\n",
    "        else:\n",
    "            bestSplitFeatIndex, bestSplitVal = chooseSplitRuleAndValue(train_data, train_labels)\n",
    "            left = [] #maybe should be ditionaries?\n",
    "            right = [] #maybe should be dictionaries?\n",
    "            for sample in range(len(train_data.shape[0])):\n",
    "                if sample[bestSplitFeatIndex] <= bestSplitValue:\n",
    "                    left.append(sample)\n",
    "                else:\n",
    "                    right.append(sample)\n",
    "            return Node(bestSplitFeatIndex, bestSplitVal,  buildTree(left), buildTree(right))                     \n",
    "    \n",
    "    def predict(test_data):\n",
    "        '''\n",
    "            Given a data point, traverse the tree to find the best label to classify the data point. \n",
    "            Start at the root node and evaluate split rules as you traverse until you reach a leaf. \n",
    "            Choose that leaf nodes label as your output label.\n",
    "            \n",
    "            test_data (ndarray) - n by d matrix of points to classify. \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "    \n",
    "    class Node:\n",
    "        def init(self, featureIndex, splitVal, rightChild, leftChild, label):\n",
    "            self.splitFeatIndex = featureIndex \n",
    "            self.splitValue = splitVal\n",
    "            self.left = leftChild\n",
    "            self.right = rightChild \n",
    "            self.label = label #if set, this node is a leaf (mode), else is intermediary\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomForest(params)\n",
    "    def __init__:\n",
    "        self.attributeBag #subset of features to train each node on in different trees\n",
    "        self.dataBag #subset of data to train each tree on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'training_labels', 'test_data', 'training_data'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SPAM DATA\n",
    "\n",
    "spamData = sp.io.loadmat(\"dist/spam_data.mat\")\n",
    "spamData.keys()\n",
    "spamTrainX = spamData['training_data']\n",
    "spamTrainY = spamData['training_labels']\n",
    "spamTest = spamData['test_data']\n",
    "\n",
    "spamTest = sklearn.preprocessing.normalize(spamTest.astype(float))\n",
    "spamTrainX = sklearn.preprocessing.normalize(spamTrainX.astype(float))\n",
    "\n",
    "combined = np.hstack([spamTrainX, spamTrainY])\n",
    "np.random.shuffle(combined)\n",
    "spamTrainX = combined[:, :-1]\n",
    "spamTrainY = combined[:, -1]\n",
    "\n",
    "#maybe add a bias feature\n",
    "\n",
    "trainX, validX, trainY, validY = train_test_split(trainX, trainY, test_size=.1, random_state=42)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [189hw]",
   "language": "python",
   "name": "Python [189hw]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
