{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Maia Rosengarten <br/>\n",
    "SID: 23572580<br/>\n",
    "March 27, 2017<br/>\n",
    "Kaggle Score:<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy as sp\n",
    "from scipy.stats import logistic as sig\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "from scipy import io\n",
    "import sklearn\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction import DictVectorizer as dv\n",
    "from sklearn.preprocessing import Imputer as imp\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NP FNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vec(arr): # d b 1 --> (d, 1)\n",
    "    return arr.reshape((arr.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def arr(vec): # 1 by d --> (d, )\n",
    "    return vec.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING FNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorizeMatrix(data):\n",
    "    '''\n",
    "        Replaces all categorical variables that take on n possible values with\n",
    "        n new columns in the design matrix. \n",
    "            e.g. If \"Weather\" takes on \"sunny\" or \"rainy\" would create two new features (columns)\n",
    "                in the design matrix, one 'sunny' and the other 'rainy'. If sample point's weather is\n",
    "                sunny, the new 'sunny' feature would take value 1, while 'rainy' would take 0.\n",
    "        Args:\n",
    "            data - a dataframe, the design matrix\n",
    "    \n",
    "    '''\n",
    "    v = DictVectorizer(sparse=False)\n",
    "    data = v.fit_transform(data.T.to_dict().values())\n",
    "    return data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preProcessData(dataDF, lstCatFeat, lstNumericFeat, lstDummies):\n",
    "    '''\n",
    "        Iterates over every value of every feature and replaces '?' (missing) with\n",
    "        feature mean (if quantitative) and feature mode (if categorical)\n",
    "        \n",
    "        Args:\n",
    "            designMatrixDF (pandas data frame) - dataframe of design matrix\n",
    "            lstCatFeatIndices (lst) - list of indices of features that are categorical\n",
    "            lstNumFeatIndices (lst) - list of indices of features that are numeric\n",
    "    \n",
    "    '''\n",
    "#     dataDF = dataDF.dropna(axis=1, how='all') #idk if works\n",
    "    dataDF.dropna(axis=0, how='all', inplace=True) #idk if works\n",
    "\n",
    "    copyDataDF = dataDF.copy()\n",
    "    for feat in lstNumericFeat:\n",
    "        copyDataDF[feat].fillna(round(dataDF[feat].mean(), 2), inplace=True)\n",
    "    for feat in lstCatFeat:\n",
    "        copyDataDF[feat].fillna(dataDF[feat].mode()[0], inplace=True)         \n",
    "    copyDataDF = pd.get_dummies(copyDataDF, columns=lstDummies)\n",
    "    return copyDataDF\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTIL FNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotCostFN(costs):\n",
    "    '''\n",
    "        Plots cost function as a function of number of iterations\n",
    "        Args:\n",
    "            costs (ndarray) - lst of costs per iteration of gradient descent\n",
    "    '''\n",
    "   \n",
    "    iters = [i for i in range(len(costs))]\n",
    "    plt.plot(iters, costs)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.title(\"Cost Per Iteration of Gradient Descent\")\n",
    "    plt.xlabel('numIterations')\n",
    "    plt.ylabel('cost')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateCsv(aryPredictions, strCsvName):\n",
    "    '''\n",
    "    Writes predictions of testSet to csv file\n",
    "    Args:\n",
    "        aryPredictions (ndarray) - (nx1)-array of predictions given size n test (or valid) set\n",
    "        strCsvName (str) - name of csv file to write to\n",
    "    '''\n",
    "    with open(strCsvName + '.csv', 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for i in range(len(aryPredictions)):\n",
    "            writer.writerow([i, aryPredictions[i]])\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, root=None, max_depth=50, leaf_condition=1):\n",
    "        self.root = root\n",
    "        self.max_depth = max_depth\n",
    "        self.leaf_condition = leaf_condition\n",
    "    \n",
    "    def train(self, train_data, train_labels):\n",
    "        '''\n",
    "            Sets the root of the decision tree to the root node of the resulting tree from buildTree.\n",
    "            Args:\n",
    "                train_data (ndarray) - cleaned training data\n",
    "                train_lablels (ndarray) - cleaned training labels\n",
    "                height_cap (int) - prevents tree from growing too large\n",
    "        \n",
    "        '''\n",
    "        node_depth = 0\n",
    "        self.root = self.buildTree(train_data, train_labels, self.max_depth, node_depth, self.leaf_condition)\n",
    "    \n",
    "    def buildTree(self, train_data, train_labels, max_depth, node_depth, leaf_condition=1):\n",
    "        '''\n",
    "            Recursively grows a decision tree by constructing nodes. Using the impurity and segmenter methods, \n",
    "            attempts to find a configuration of nodes that best splits the input data. \n",
    "            This function figures out the split rules that each node should have and figures out \n",
    "            when to stop growing the tree and insert a leaf node.  \n",
    "            \n",
    "            This phase is training the data.\n",
    "            \n",
    "            Args:\n",
    "                train_data (ndarray) - n by d matrix of cleaned training data\n",
    "                train_labels (ndarray) - n by 1 vector of training labels\n",
    "                max_depth (int) - caps the height of the tree to prevent it from growing too large\n",
    "                \n",
    "        '''\n",
    "        #fix this stopping condition\n",
    "        numSamples = train_data.shape[0]\n",
    "        numClass1 = np.sum(train_labels)\n",
    "        numClass0 = numSamples - numClass1\n",
    "        \n",
    "#         #is leaf\n",
    "        isLeaf, label = self.isMajClassLeaf(numSamples, numClass1, numClass0, leaf_condition)\n",
    "        if (isLeaf):\n",
    "            return Node(label=label)\n",
    "        elif (max_depth==0): #isLeaf\n",
    "            if (numClass1 > numClass0):\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "            return Node(label=label)  \n",
    "        else:\n",
    "            node = Node()\n",
    "            node.chooseSplitRule(train_data, train_labels)\n",
    "            \n",
    "            leftIndices = [] \n",
    "            rightIndices = [] \n",
    "\n",
    "            for i in range(train_data.shape[0]):\n",
    "                if (train_data[i, node.splitFeatIndex] <= node.splitVal):\n",
    "                    leftIndices.append(i)\n",
    "                else:\n",
    "                    rightIndices.append(i)\n",
    "            \n",
    "            leftSetX = train_data[leftIndices, :]\n",
    "            leftSetY = train_labels[leftIndices]\n",
    "\n",
    "            \n",
    "            rightSetX = train_data[rightIndices, :]\n",
    "            rightSetY= train_labels[rightIndices]\n",
    "\n",
    "            sizeRightSet = len(rightSetX)\n",
    "            sizeLeftSet = len(leftSetX)\n",
    "            \n",
    "            if (sizeRightSet==0):\n",
    "                if (np.sum(rightSetY) > sizeRightSet/2):\n",
    "                    return Node(label=1)\n",
    "                return Node(label=0)\n",
    "            \n",
    "            if (sizeLeftSet==0):\n",
    "                if (np.sum(leftSetY)> sizeLeftSet/2):\n",
    "                    return Node(label=1)\n",
    "                return Node(label=0)\n",
    "                \n",
    "            max_depth -=1\n",
    "            node_depth+=1\n",
    "            return Node(featureIndex=node.splitFeatIndex, splitVal=node.splitVal, leftChild=self.buildTree(leftSetX, leftSetY, max_depth, node_depth, leaf_condition), rightChild=self.buildTree(rightSetX, rightSetY, max_depth, node_depth, leaf_condition), depth=node_depth)\n",
    "    \n",
    "        \n",
    "    def predictPoint(self, node, data_point):\n",
    "        '''\n",
    "            Given a data point, traverse the tree to find the best label to classify the data point. \n",
    "            Start at the root node and evaluate split rules as you traverse until you reach a leaf. \n",
    "            Choose that leaf nodes label as your output label.\n",
    "            \n",
    "            data_point (ndarray) - (dx1) single point to classify with d features \n",
    "        \n",
    "        '''\n",
    "        if (node is None):\n",
    "            print('Error: root in tree is None')\n",
    "            return None\n",
    "        if (node.label is not None):\n",
    "            tortn = node.label\n",
    "            return tortn  \n",
    "#         print('data[node.splitFeatIndex] ' + str(data_point[node.splitFeatIndex]))\n",
    "#         print('node.splitVal ' + str(node.splitVal))\n",
    "        if (data_point[node.splitFeatIndex] <= node.splitVal):\n",
    "            return self.predictPoint(node.left, data_point)\n",
    "        else:\n",
    "            return self.predictPoint(node.right, data_point)\n",
    "    \n",
    "    def predict(self, dataMatrix):\n",
    "        '''\n",
    "            Given a matrix of test samples, predict labels.\n",
    "            \n",
    "            dataMatrix (ndarray) - (nxd) unseen samples with d dimensions \n",
    "        ''' \n",
    "        predictions = []\n",
    "        for i in range(dataMatrix.shape[0]):   \n",
    "            label = self.predictPoint(self.root, dataMatrix[i])\n",
    "            predictions.append(label)\n",
    "        return predictions\n",
    "    \n",
    "    def isMajClassLeaf(self, numSamples, numClass1, numClass0, threshold):\n",
    "        '''\n",
    "            Returns (True, y) if threshold% of points in a node are of a particular class\n",
    "            where y is the label of the leaf.\n",
    "\n",
    "        '''     \n",
    "        if (numClass1 >= (threshold * numSamples)):\n",
    "            return True, 1\n",
    "        elif (numClass0 >= (threshold * numSamples)):\n",
    "            return True, 0\n",
    "        return False, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. DECISION TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, numTrees, train_data, train_labels, leaf_condition):\n",
    "        self.train_data = train_data\n",
    "        self.train_labels = train_labels\n",
    "        self.trees = []\n",
    "        self.numTrees = numTrees\n",
    "        self.leaf_condition = leaf_condition\n",
    "        for i in range(self.numTrees):\n",
    "            self.trees.append(DecisionTree(leaf_condition=self.leaf_condition))          \n",
    "                     \n",
    "    def train(self, train_data):\n",
    "        i = 0\n",
    "        for tree in self.trees:\n",
    "            print('tree ' + str(i))\n",
    "            numSamples = self.train_data.shape[0]\n",
    "            random = np.random.choice(numSamples, numSamples, replace=True)\n",
    "            subset_data = self.train_data[random]\n",
    "            subset_labels = self.train_labels[random]\n",
    "            tree.train(subset_data, subset_labels)\n",
    "            i+=1\n",
    "    \n",
    "    def predict(self, valid_data):\n",
    "        predictions = []\n",
    "        for point in valid_data:\n",
    "            labels = []\n",
    "            for tree in self.trees:\n",
    "                labels.append(tree.predictPoint(tree.root, point))\n",
    "            if (np.sum(labels) >= 0.5 * self.numTrees):\n",
    "                predictions.append(1)\n",
    "            else:\n",
    "                predictions.append(0)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through all the spam data\n",
    "Bag of words: have b words in vocabulary and vector is in R^b. vi = 1 if word i appears in document\n",
    "if vocabulary is \"ham\", \"spam\", \"sam\" then you have your sentences: \"sam likes spam\": [0, 1, 1]...\n",
    "\n",
    "When taking mutually exclusive sets, can overfit. Writing a better featurize.py\n",
    "Take most commonly occurring words and make it an exclusive set. I get set of all words minus the stuff that the \n",
    "two classes have in common (like \"the\"). vocab of one class, vocab of two class. and can do set(v1) - set(v2) (but can overfit)...\n",
    "\n",
    "if that can overfit, then can add other words from in between randomly. Or dimensionality reduction on total bag of words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, featureIndex=0, splitVal=None, leftChild=None, rightChild=None, label=None, depth=1):\n",
    "        self.splitFeatIndex = featureIndex \n",
    "        self.splitVal = splitVal\n",
    "        self.left = leftChild\n",
    "        self.right = rightChild \n",
    "        self.label = label \n",
    "        self.depth = depth\n",
    "    \n",
    "    \n",
    "    def chooseSplitRule(self, train_data, train_labels):\n",
    "        '''\n",
    "            Finds the best split rule for a Node using the impurity measure and input data. \n",
    "            Exhaustively tries threshold values from the data and chooses the combination of \n",
    "            split feature and threshold with the lowest impurity value. \n",
    "           \n",
    "            Args:\n",
    "               train_data (ndarray) - cleaned train data \n",
    "               train_labels (ndarray) - train labels\n",
    "            \n",
    "            Returns (bestSplitFeatIndex, bestSplitValue)\n",
    "                 \n",
    "            \n",
    "            If xi is quantitative, sort points in S by feature xi; \n",
    "            remove duplicates; [duplicate values, not points]\n",
    "            try splitting between each pair of consecutive values.\n",
    "            [We can radix sort the values in linear time, and if n is huge we should.]\n",
    "            \n",
    "            Clever Bit: As you scan sorted list from left to right, \n",
    "            you can update entropy in O(1) time per point! \n",
    "            [This is important for obtaining a fast tree-building time.] <--GO OVER THIS.\n",
    "            \n",
    "            Need to deal with case when a child is empty (skip that split)\n",
    "    \n",
    "        '''\n",
    "        #<= split value --> left, > split value --> right\n",
    "        \n",
    "        currNodeSize = len(train_data)\n",
    "        currClass1Count = sum(train_labels) \n",
    "        currClass0Count = currNodeSize - currClass1Count\n",
    "        currNodeEntropy = self.computeEntropy((currNodeSize, currClass0Count, currClass1Count))\n",
    "        bestInfoGain = -float(\"inf\")\n",
    " \n",
    "        labelDictRight = {0: currClass0Count, 1: currClass1Count}\n",
    "        labelDictLeft = {0:0, 1:0}\n",
    "        sizeRightSet = currNodeSize\n",
    "        sizeLeftSet = 0\n",
    "        \n",
    "        for featureIndex in range(train_data.shape[1]):\n",
    "            sortedIndices = np.argsort(train_data[:, featureIndex])\n",
    "            sortedVals = np.sort(train_data[:, featureIndex])\n",
    "            \n",
    "            lstUniqueVal, lstUniqueCounts = np.unique(sortedVals, return_counts=True)\n",
    "            runningIndex = - 1\n",
    "            for i in range(len(lstUniqueVal)):\n",
    "                runningIndex += (lstUniqueCounts[i])\n",
    "                tempSplitVal = lstUniqueVal[i]\n",
    "                \n",
    "                sizeLeftSet = runningIndex + 1\n",
    "                sizeRightSet = currNodeSize - sizeLeftSet\n",
    "                \n",
    "                labelDictLeft[1] = np.sum(train_labels[sortedIndices[:runningIndex+1]])\n",
    "                labelDictRight[1] = np.sum(train_labels[sortedIndices[runningIndex+1:]])\n",
    "                labelDictRight[0] = sizeRightSet - labelDictRight[1]\n",
    "                labelDictLeft[0] = sizeLeftSet - labelDictLeft[1]\n",
    "                \n",
    "                tupSizeLeftClassCounts = (sizeLeftSet, labelDictLeft[0], labelDictLeft[1])\n",
    "                tupSizeRightClassCounts = (sizeRightSet, labelDictRight[0], labelDictRight[1])\n",
    "                entropyAfterSplit = self.entropyAfterSplit(tupSizeLeftClassCounts, tupSizeRightClassCounts) \n",
    "                infoGain = currNodeEntropy - entropyAfterSplit\n",
    "                \n",
    "                if infoGain > bestInfoGain:\n",
    "                    bestInfoGain = infoGain\n",
    "                    bestSplitVal = tempSplitVal\n",
    "                    bestSplitIndex = featureIndex \n",
    "  \n",
    "        self.splitFeatIndex = bestSplitIndex \n",
    "        self.splitVal = bestSplitVal\n",
    "              \n",
    "    def entropyAfterSplit(self, tupSizeLeftClassCounts, tupSizeRightClassCounts):\n",
    "        '''\n",
    "            A method that takes in the result of a split: two histograms\n",
    "            that count the frequencies of labels on the ”left” and ”right” side of that split.\n",
    "            Calculates and outputs a scalar value representing the impurity (”badness”) of the \n",
    "            specified split on the input data. In lecture, this was refered to as H_after where H\n",
    "            is the function that computes entropy.\n",
    "            \n",
    "            Args:\n",
    "                tupSizeLeftClassCounts (tup) - \n",
    "                tupSizeRightClassCounts (tup) - \n",
    "        \n",
    "        '''\n",
    "        sizeLeftSet = tupSizeLeftClassCounts[0]\n",
    "        sizeRightSet = tupSizeRightClassCounts[0]\n",
    "\n",
    "        entropyLeft = self.computeEntropy(tupSizeLeftClassCounts)\n",
    "        entropyRight = self.computeEntropy(tupSizeRightClassCounts)\n",
    "        entropyAfterSplit = (entropyLeft * sizeLeftSet + entropyRight * sizeRightSet)/ (sizeLeftSet + sizeRightSet)\n",
    "        return entropyAfterSplit\n",
    "    \n",
    "    \n",
    "    def computeEntropy(self, tupleSizeSetClassCounts):\n",
    "        '''\n",
    "            Compute the entropy of a set (assume 2 classes). \n",
    "            \n",
    "            Args:\n",
    "                tupleSetSizeClassSize (tup) - (cardinality of set, cardinality of class 0 in set) \n",
    "        '''\n",
    "        sizeSet, countClass0, countClass1 = tupleSizeSetClassCounts\n",
    "        if (sizeSet==0):\n",
    "            return 0\n",
    "        probClass0 = countClass0/sizeSet\n",
    "        if (probClass0 == 1 or probClass0 == 0):\n",
    "            return 0\n",
    "        else: \n",
    "            entropy = (- probClass0 * np.log2(probClass0)) - ((1 - probClass0) * np.log2(1 - probClass0))\n",
    "        return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) SPAM DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spamData = sp.io.loadmat(\"dist/spam_data.mat\")\n",
    "spamData.keys()\n",
    "spamTrainX = spamData['training_data']\n",
    "spamTrainY = spamData['training_labels']\n",
    "spamTest = spamData['test_data']\n",
    "\n",
    "combined = np.hstack([spamTrainX, spamTrainY.T])\n",
    "np.random.shuffle(combined)\n",
    "combined = imp().fit_transform(combined)\n",
    "\n",
    "spamTrainX = combined[:, :-1]\n",
    "spamTrainY = combined[:, -1]\n",
    "\n",
    "trainX, validX, trainY, validY = train_test_split(spamTrainX, spamTrainY, test_size=.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spamDecision = DecisionTree(max_depth=50, leaf_condition=.84)\n",
    "spamDecision.train(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validPredictionsSpam = spamDecision.predict(validX)\n",
    "validAccuracySpam = accuracy_score(validY, validPredictionsSpam)\n",
    "\n",
    "trainPredictionsSpam = spamDecision.predict(trainX)\n",
    "trainAccuracySpam = accuracy_score(trainY, trainPredictionsSpam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testPredictionsSpam = spamDecision.predict(spamTest)\n",
    "generateCsv(testPredictionsSpam, 'sundaySpam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Forrest Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_trees = 50\n",
    "leaf_condition = 0.84\n",
    "randForestSpam = RandomForest(num_trees, trainX, trainY, leaf_condition)\n",
    "randForestSpam.train(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validPredictionsSpamForest = randForestSpam.predict(validX)\n",
    "validAccuracySpamForest = accuracy_score(validY, validPredictionsSpamForest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainPredictionsSpamForest = randForestSpam.predict(trainX)\n",
    "trainAccuracySpamForest = accuracy_score(trainY, trainPredictionsSpamForest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Evaluatoin: Tree vs Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPAM DECISION TREE:\n",
      "validation accuracy: 78.62%\n",
      "training accuracy: 79.45%\n",
      "\n",
      "\n",
      "SPAM DECISION FOREST:\n",
      "validation accuracy 79.29%\n",
      "training accuracy 79.4%\n"
     ]
    }
   ],
   "source": [
    "print(\"SPAM DECISION TREE:\")\n",
    "print(\"validation accuracy: \" + str(round(validAccuracySpam * 100, 2)) + \"%\")\n",
    "print(\"training accuracy: \" + str(round(trainAccuracySpam * 100, 2)) + \"%\")\n",
    "print(\"\\n\")\n",
    "print(\"SPAM DECISION FOREST:\")\n",
    "print(\"validation accuracy \" + str(round(validAccuracySpamForest * 100, 2)) + \"%\")\n",
    "print(\"training accuracy \" + str(round(trainAccuracySpamForest * 100, 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanicTest = pd.read_csv(\"hw5_titanic_dist/titanic_testing_data.csv\")\n",
    "titanicTrain = pd.read_csv(\"hw5_titanic_dist/titanic_training.csv\")\n",
    "\n",
    "titanicLabels = titanicTrain.iloc[0:, 0:1]\n",
    "titanicTrain = titanicTrain.loc[0:, [\"pclass\", \"sex\", \"age\", \"sibsp\", \"parch\", \"ticket\", \"fare\", \"cabin\", \"embarked\"]]\n",
    "\n",
    "headers = vec(np.array(list(titanicTrain))).T\n",
    "lstNumericFeat = arr(headers[:, [2, 5, 6]])\n",
    "lstCatFeat = arr(headers[:, [0, 1, 3, 4, 8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanicTrain['ticket'].replace(regex=True,inplace=True,to_replace=r'\\D',value=r'')\n",
    "titanicTest['ticket'].replace(regex=True,inplace=True,to_replace=r'\\D',value=r'')\n",
    "titanicTrain.drop('cabin', axis=1, inplace=True)\n",
    "titanicTest.drop('cabin', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexToElim = titanicTrain.index[titanicTrain.isnull().all(axis=1)][0]\n",
    "titanicLabels = titanicLabels.drop([705])\n",
    "\n",
    "lstDummies = [\"sex\", \"embarked\"]\n",
    "titanicTrain = preProcessData(titanicTrain, lstCatFeat, lstNumericFeat, lstDummies)\n",
    "titanicTest = preProcessData(titanicTest, lstCatFeat, lstNumericFeat, lstDummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanicTrain = titanicTrain.as_matrix()\n",
    "titanicLabels = titanicLabels.as_matrix()\n",
    "titanicTest = titanicTest.as_matrix()\n",
    "\n",
    "combined = np.hstack([titanicTrain, titanicLabels])\n",
    "np.random.shuffle(combined)\n",
    "titanicTrainX = combined[:, :-1]\n",
    "titanicTrainY = combined[:, -1]\n",
    "\n",
    "trainX, validX, trainY, validY = train_test_split(titanicTrainX, titanicTrainY, test_size=.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanicTree = DecisionTree(max_depth=50, leaf_condition=.84)\n",
    "titanicTree.train(trainX, trainY)\n",
    "\n",
    "validPredictionsTitanic = titanicTree.predict(validX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstCorrectLabelsValid = [1 for i in range(len(validY)) if validY[i]==validPredictionsTitanic[i]]\n",
    "validAccuracyTitanic = np.sum(lstCorrectLabelsValid)/len(validY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainPredictionsTitanic = titanicTree.predict(trainX)\n",
    "lstCorrectLabelsTrain = [1 for i in range(len(trainY)) if trainY[i]==trainPredictionsTitanic[i]]\n",
    "trainAccuracyTitanic = np.sum(lstCorrectLabelsTrain)/len(trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Forrest Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_trees = 50\n",
    "leaf_condition = 0.84\n",
    "randForestTitanic = RandomForest(num_trees, trainX, trainY, leaf_condition)\n",
    "randForestTitanic.train(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validPredictionsTitanicForest = randForestTitanic.predict(validX)\n",
    "lstCorrectLabelsForest = [1 for i in range(len(validY)) if validY[i]==validPredictionsTitanicForest[i]]\n",
    "validAccuracyTitanicForest = np.sum(lstCorrectLabelsForest)/len(validY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainPredictionsTitanicForest = randForestTitanic.predict(trainX)\n",
    "lstCorrectLabelsForest = [1 for i in range(len(trainY)) if trainY[i]==trainPredictionsTitanicForest[i]]\n",
    "trainAccuracyTitanicForest = np.sum(lstCorrectLabelsForest)/len(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITANIC DECISION TREE:\n",
      "validation accuracy: 78.0%\n",
      "training accuracy: 92.32%\n",
      "\n",
      "\n",
      "TITANIC DECISION FOREST:\n",
      "validation accuracy 81.0%\n",
      "training accuracy 90.21%\n"
     ]
    }
   ],
   "source": [
    "print(\"TITANIC DECISION TREE:\")\n",
    "print(\"validation accuracy: \" + str(round(validAccuracyTitanic * 100, 2)) + \"%\")\n",
    "print(\"training accuracy: \" + str(round(trainAccuracyTitanic * 100, 2)) + \"%\")\n",
    "print(\"\\n\")\n",
    "print(\"TITANIC DECISION FOREST:\")\n",
    "print(\"validation accuracy \" + str(round(validAccuracyTitanicForest * 100, 2)) + \"%\")\n",
    "print(\"training accuracy \" + str(round(trainAccuracyTitanicForest * 100, 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle <br/>\n",
    "Score: 83.87% <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testPredictionsTitanicTree = titanicTree.predict(titanicTest)\n",
    "generateCsv(testPredictionsTitanicTree, 'sundayTitanic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "censusTest = pd.read_csv(\"hw5_census_dist/test_data.csv\")\n",
    "censusTrain = pd.read_csv(\"hw5_census_dist/train_data.csv\")\n",
    "\n",
    "censusLabels = censusTrain.iloc[:, -1].to_frame()\n",
    "censusTrain = censusTrain.loc[:, [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\"]]\n",
    "\n",
    "headers = vec(np.array(list(censusTrain))).T\n",
    "lstNumericFeat = arr(headers[:, [0, 2, 4, 10, 11, 12]])\n",
    "lstCatFeat = arr(headers[:, [1, 3, 5, 6, 7, 8, 9, 13]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstDummies = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"]\n",
    "censusTrain = preProcessData(censusTrain, lstCatFeat, lstNumericFeat, lstDummies)\n",
    "censusTest = preProcessData(censusTest, lstCatFeat, lstNumericFeat, lstDummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "censusTrain = censusTrain.as_matrix()\n",
    "censusLabels = censusLabels.as_matrix()\n",
    "censusTest = censusTest.as_matrix()\n",
    "\n",
    "combined = np.hstack([censusTrain, censusLabels])\n",
    "np.random.shuffle(combined)\n",
    "\n",
    "censusTrainX = combined[:, :-1]\n",
    "censusTrainY = combined[:, -1]\n",
    "\n",
    "trainX, validX, trainY, validY = train_test_split(censusTrainX, censusTrainY, test_size=.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "censusTree = DecisionTree(max_depth=50, leaf_condition=.84)\n",
    "censusTree.train(trainX, trainY)\n",
    "\n",
    "validPredictionsCensus = censusTree.predict(validX)\n",
    "validAccuracyCensus = accuracy_score(validY, validPredictionsCensus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainPredictionsCensus = censusTree.predict(trainX)\n",
    "trainAccuracyCensus = accuracy_score(trainY, trainPredictionsCensus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testPredictionsCensusTree = censusTree.predict(censusTest)\n",
    "generateCsv(testPredictionsCensusTree, 'sundayCensus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree 0\n",
      "tree 1\n",
      "tree 2\n",
      "tree 3\n",
      "tree 4\n",
      "tree 5\n",
      "tree 6\n",
      "tree 7\n",
      "tree 8\n",
      "tree 9\n"
     ]
    }
   ],
   "source": [
    "num_trees = 10\n",
    "leaf_condition = 0.84\n",
    "randForestCensus = RandomForest(num_trees, trainX, trainY, leaf_condition)\n",
    "randForestCensus.train(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validPredictionsCensusForest = randForestCensus.predict(validX)\n",
    "validAccuracyCensusForest = accuracy_score(validY, validPredictionsCensusForest)\n",
    "\n",
    "trainPredictionsCensusForest = randForestCensus.predict(trainX)\n",
    "trainAccuracyCensusForest = accuracy_score(trainY, trainPredictionsCensusForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CENSUS DECISION TREE:\n",
      "validation accuracy: 84.05%\n",
      "training accuracy: 92.84%\n",
      "\n",
      "\n",
      "CENSUS DECISION FOREST:\n",
      "validation accuracy 81.0%\n",
      "training accuracy 90.21%\n"
     ]
    }
   ],
   "source": [
    "print(\"CENSUS DECISION TREE:\")\n",
    "print(\"validation accuracy: \" + str(round(validAccuracyCensus * 100, 2)) + \"%\")\n",
    "print(\"training accuracy: \" + str(round(trainAccuracyCensus * 100, 2)) + \"%\")\n",
    "print(\"\\n\")\n",
    "print(\"CENSUS DECISION FOREST:\")\n",
    "print(\"validation accuracy \" + str(round(validAccuracyTitanicForest * 100, 2)) + \"%\")\n",
    "print(\"training accuracy \" + str(round(trainAccuracyTitanicForest * 100, 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [189hw]",
   "language": "python",
   "name": "Python [189hw]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
