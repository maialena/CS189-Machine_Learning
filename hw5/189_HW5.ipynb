{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Maia Rosengarten <br/>\n",
    "SID: 23572580<br/>\n",
    "March 27, 2017<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy as sp\n",
    "from scipy.stats import logistic as sig\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "from scipy import io\n",
    "import sklearn\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction import DictVectorizer as dv\n",
    "from sklearn.preprocessing import Imputer as imp\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NP FNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vec(arr): # d b 1 --> (d, 1)\n",
    "    return arr.reshape((arr.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def arr(vec): # 1 by d --> (d, )\n",
    "    return vec.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING FNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preProcessData(dataDF, lstCatFeat, lstNumericFeat, lstDummies):\n",
    "    '''\n",
    "        Iterates over every value of every feature and replaces '?' (missing) with\n",
    "        feature mean (if quantitative) and feature mode (if categorical)\n",
    "        \n",
    "        Args:\n",
    "            designMatrixDF (pandas data frame) - dataframe of design matrix\n",
    "            lstCatFeatIndices (lst) - list of indices of features that are categorical\n",
    "            lstNumFeatIndices (lst) - list of indices of features that are numeric\n",
    "    \n",
    "    '''\n",
    "#     dataDF = dataDF.dropna(axis=1, how='all') #idk if works\n",
    "    dataDF.dropna(axis=0, how='all', inplace=True) #idk if works\n",
    "\n",
    "    copyDataDF = dataDF.copy()\n",
    "    for feat in lstNumericFeat:\n",
    "        copyDataDF[feat].fillna(round(dataDF[feat].mean(), 2), inplace=True)\n",
    "    for feat in lstCatFeat:\n",
    "        copyDataDF[feat].fillna(dataDF[feat].mode()[0], inplace=True)         \n",
    "    copyDataDF = pd.get_dummies(copyDataDF, columns=lstDummies)\n",
    "    return copyDataDF\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTIL FNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotAccuracies(accuracies, depths):\n",
    "    '''\n",
    "        Plots accuracies as a function od depth\n",
    "        Args:\n",
    "            costs (ndarray) - lst of costs per iteration of gradient descent\n",
    "    '''\n",
    "   \n",
    "    plt.plot(depths, accuracies)\n",
    "    plt.title(\"Prediction Evaluation Decision Tree On Census Data\")\n",
    "    plt.xlabel('max_depth')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateCsv(aryPredictions, strCsvName):\n",
    "    '''\n",
    "    Writes predictions of testSet to csv file\n",
    "    Args:\n",
    "        aryPredictions (ndarray) - (nx1)-array of predictions given size n test (or valid) set\n",
    "        strCsvName (str) - name of csv file to write to\n",
    "    '''\n",
    "    with open(strCsvName + '.csv', 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for i in range(len(aryPredictions)):\n",
    "            writer.writerow([i, aryPredictions[i]])\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. DECISION TREE IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, root=None, max_depth=50, leaf_condition=1):\n",
    "        self.root = root\n",
    "        self.max_depth = max_depth\n",
    "        self.leaf_condition = leaf_condition\n",
    "    \n",
    "    def train(self, train_data, train_labels, sizeFeatSubset):\n",
    "        '''\n",
    "            Sets the root of the decision tree to the root node of the resulting tree from buildTree.\n",
    "            Args:\n",
    "                train_data (ndarray) - cleaned training data\n",
    "                train_lablels (ndarray) - cleaned training labels\n",
    "                height_cap (int) - prevents tree from growing too large\n",
    "        \n",
    "        '''\n",
    "        node_depth = 0\n",
    "        self.root = self.buildTree(train_data, train_labels, self.max_depth, node_depth, sizeFeatSubset, self.leaf_condition)\n",
    "    \n",
    "    def buildTree(self, train_data, train_labels, max_depth, node_depth, sizeFeatSubset, leaf_condition=1):\n",
    "        '''\n",
    "            Recursively grows a decision tree by constructing nodes. Using the impurity and segmenter methods, \n",
    "            attempts to find a configuration of nodes that best splits the input data. \n",
    "            This function figures out the split rules that each node should have and figures out \n",
    "            when to stop growing the tree and insert a leaf node.  \n",
    "            \n",
    "            This phase is training the data.\n",
    "            \n",
    "            Args:\n",
    "                train_data (ndarray) - n by d matrix of cleaned training data\n",
    "                train_labels (ndarray) - n by 1 vector of training labels\n",
    "                max_depth (int) - caps the height of the tree to prevent it from growing too large\n",
    "                \n",
    "        '''\n",
    "        numSamples = train_data.shape[0]\n",
    "        numFeatures = train_data.shape[1]\n",
    "        numClass1 = np.sum(train_labels)\n",
    "        numClass0 = numSamples - numClass1\n",
    "        \n",
    "        isLeaf, label = self.isMajClassLeaf(numSamples, numClass1, numClass0, leaf_condition)\n",
    "        if (isLeaf):\n",
    "            return Node(label=label)\n",
    "        elif (max_depth==0): \n",
    "            if (numClass1 > numClass0):\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "            return Node(label=label)  \n",
    "        else:\n",
    "            node = Node()\n",
    "            node.chooseSplitRule(train_data, train_labels, sizeFeatSubset)\n",
    "            leftIndices = [] \n",
    "            rightIndices = [] \n",
    "\n",
    "            for i in range(train_data.shape[0]):\n",
    "                if (train_data[i, node.splitFeatIndex] <= node.splitVal):\n",
    "                    leftIndices.append(i)\n",
    "                else:\n",
    "                    rightIndices.append(i)\n",
    "            \n",
    "            leftSetX = train_data[leftIndices, :]\n",
    "            leftSetY = train_labels[leftIndices]\n",
    "\n",
    "            \n",
    "            rightSetX = train_data[rightIndices, :]\n",
    "            rightSetY= train_labels[rightIndices]\n",
    "\n",
    "            sizeRightSet = len(rightSetX)\n",
    "            sizeLeftSet = len(leftSetX)\n",
    "            \n",
    "            if (sizeRightSet==0):\n",
    "                if (np.sum(rightSetY) > sizeRightSet/2):\n",
    "                    return Node(label=1)\n",
    "                return Node(label=0)\n",
    "            \n",
    "            if (sizeLeftSet==0):\n",
    "                if (np.sum(leftSetY)> sizeLeftSet/2):\n",
    "                    return Node(label=1)\n",
    "                return Node(label=0)\n",
    "                \n",
    "            max_depth -=1\n",
    "            node_depth+=1\n",
    "            return Node(featureIndex=node.splitFeatIndex, splitVal=node.splitVal, leftChild=self.buildTree(leftSetX, leftSetY, max_depth, node_depth, sizeFeatSubset, leaf_condition), rightChild=self.buildTree(rightSetX, rightSetY, max_depth, node_depth, sizeFeatSubset, leaf_condition), parentNode=node, depth=node_depth)\n",
    "    \n",
    "         \n",
    "    def predictPointWithPrint(self, node, data_point, lstFeatures):\n",
    "        '''\n",
    "            Given a data point, traverse the tree to find the best label to classify the data point. \n",
    "            Start at the root node and evaluate split rules as you traverse until you reach a leaf. \n",
    "            Choose that leaf nodes label as your output label.\n",
    "            \n",
    "            data_point (ndarray) - (dx1) single point to classify with d features \n",
    "        \n",
    "        '''\n",
    "        if (node is None):\n",
    "            print('Error: root in tree is None')\n",
    "            return None\n",
    "        if (node.label is not None):\n",
    "            tortn = node.label\n",
    "            print('Therefore the class is ' + str(tortn) + \"\\n\")\n",
    "            return tortn  \n",
    "        if (data_point[node.splitFeatIndex] <= node.splitVal):\n",
    "            print(\"('\" + str(lstFeatures[node.splitFeatIndex] + \"') <= \" + str(node.splitVal)))\n",
    "            return self.predictPointWithPrint(node.left, data_point, lstFeatures)\n",
    "        else:\n",
    "            print(\"('\" + str(lstFeatures[node.splitFeatIndex] + \"') > \" + str(node.splitVal)))      \n",
    "            return self.predictPointWithPrint(node.right, data_point, lstFeatures)\n",
    "    \n",
    "    def predictWithPrint(self, dataMatrix, dataLabels, lstFeatures):\n",
    "        '''\n",
    "            Given a matrix of test samples, predict labels.\n",
    "            \n",
    "            dataMatrix (ndarray) - (nxd) unseen samples with d dimensions \n",
    "        ''' \n",
    "        predictions = []\n",
    "        for i in range(dataMatrix.shape[0]):\n",
    "            print(\"Predicting sample\" + str(i) + \": \")\n",
    "            print(\"Actual class: \" + str(dataLabels[i]))\n",
    "            label = self.predictPointWithPrint(self.root, dataMatrix[i], lstFeatures)\n",
    "            predictions.append(label)\n",
    "        return predictions\n",
    "                    \n",
    "    \n",
    "    def predictPoint(self, node, data_point):\n",
    "        '''\n",
    "            Given a data point, traverse the tree to find the best label to classify the data point. \n",
    "            Start at the root node and evaluate split rules as you traverse until you reach a leaf. \n",
    "            Choose that leaf nodes label as your output label.\n",
    "            \n",
    "            data_point (ndarray) - (dx1) single point to classify with d features \n",
    "        \n",
    "        '''\n",
    "        if (node is None):\n",
    "            print('Error: root in tree is None')\n",
    "            return None\n",
    "        if (node.label is not None):\n",
    "            return node.label  \n",
    "        if (data_point[node.splitFeatIndex] <= node.splitVal):\n",
    "            return self.predictPoint(node.left, data_point)\n",
    "        else:\n",
    "            return self.predictPoint(node.right, data_point)\n",
    "    \n",
    "    def predict(self, dataMatrix):\n",
    "        '''\n",
    "            Given a matrix of test samples, predict labels.\n",
    "            \n",
    "            dataMatrix (ndarray) - (nxd) unseen samples with d dimensions \n",
    "        ''' \n",
    "        predictions = []\n",
    "        for i in range(dataMatrix.shape[0]):\n",
    "            label = self.predictPoint(self.root, dataMatrix[i])\n",
    "            predictions.append(label)\n",
    "        return predictions\n",
    "    \n",
    "    def isMajClassLeaf(self, numSamples, numClass1, numClass0, threshold):\n",
    "        '''\n",
    "            Returns (True, y) if threshold% of points in a node are of a particular class\n",
    "            where y is the label of the leaf.\n",
    "\n",
    "        '''     \n",
    "        if (numClass1 >= (threshold * numSamples)):\n",
    "            return True, 1\n",
    "        elif (numClass0 >= (threshold * numSamples)):\n",
    "            return True, 0\n",
    "        return False, None\n",
    "    \n",
    "    def visualize_tree(self, filename):\n",
    "        graph = pydot.Dot(graph_type=\"graph\")\n",
    "        visited, queue = set(), [self.root]\n",
    "        while queue:\n",
    "            node = queue.pop()\n",
    "            try:\n",
    "                if node not in visited:\n",
    "                    visited.add(node)\n",
    "                    if node.label is None:\n",
    "                        parent_node = pydot.Node(str(node.splitFeatIndex + \" : \" + node.splitVal))\n",
    "                        if node.left is not None:\n",
    "                            if node.left.label is not None:\n",
    "                                left_label = node.left.label\n",
    "                            else:\n",
    "                                left_label = str(node.left.splitFeatIndex + \" : \" + node.left.splitVal)\n",
    "                            left_node = pydot.Node(left_label)\n",
    "                            queue.append(left_node)\n",
    "                            graph.add_edge(pydot.Edge(parent_node, left_node))\n",
    "                        if node.right is not None:\n",
    "                            if node.right.label is not None:\n",
    "                                right_label = node.right.label\n",
    "                            else:\n",
    "                                right_label = str(node.right.splitFeatIndex + \" : \" + node.right.splitVal)\n",
    "                            right_node = pydot.Node(right_label)\n",
    "                            queue.append(right_node)\n",
    "                            graph.add_edge(pydot.Edge(parent_node, right_node))\n",
    "            except:\n",
    "                pass\n",
    "        graph.write_png(filename + '.png')\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. RANDOM FOREST IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, numTrees, train_data, train_labels, leaf_condition):\n",
    "        self.train_data = train_data\n",
    "        self.train_labels = train_labels\n",
    "        self.trees = []\n",
    "        self.numTrees = numTrees\n",
    "        self.leaf_condition = leaf_condition\n",
    "        for i in range(self.numTrees):\n",
    "            self.trees.append(DecisionTree(leaf_condition=self.leaf_condition))          \n",
    "                     \n",
    "    def train(self, train_data, sizeFeatSubset):\n",
    "        for tree in self.trees:\n",
    "            numSamples = self.train_data.shape[0]\n",
    "            randomSamples = np.random.choice(numSamples, numSamples, replace=True)\n",
    "            subset_data = self.train_data[randomSamples]\n",
    "            subset_labels = self.train_labels[randomSamples]\n",
    "            tree.train(subset_data, subset_labels, sizeFeatSubset)\n",
    "    \n",
    "    def predict(self, valid_data):\n",
    "        predictions = []\n",
    "        for point in valid_data:\n",
    "            labels = []\n",
    "            for tree in self.trees:\n",
    "                labels.append(tree.predictPoint(tree.root, point))\n",
    "            if (np.sum(labels) >= 0.5 * self.numTrees):\n",
    "                predictions.append(1)\n",
    "            else:\n",
    "                predictions.append(0)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, featureIndex=0, splitVal=None, leftChild=None, rightChild=None, parentNode=None, label=None, depth=1):\n",
    "        self.splitFeatIndex = featureIndex \n",
    "        self.splitVal = splitVal\n",
    "        self.left = leftChild\n",
    "        self.right = rightChild\n",
    "        self.parent = parentNode\n",
    "        self.label = label \n",
    "        self.depth = depth\n",
    "    \n",
    "    \n",
    "    def chooseSplitRule(self, train_data, train_labels, sizeFeatSubset):\n",
    "        '''\n",
    "            Finds the best split rule for a Node using the impurity measure and input data. \n",
    "            Exhaustively tries threshold values from the data and chooses the combination of \n",
    "            split feature and threshold with the lowest impurity value. \n",
    "           \n",
    "            Args:\n",
    "               train_data (ndarray) - cleaned train data \n",
    "               train_labels (ndarray) - train labels\n",
    "            \n",
    "            Returns (bestSplitFeatIndex, bestSplitValue)\n",
    "                 \n",
    "            \n",
    "            If xi is quantitative, sort points in S by feature xi; \n",
    "            remove duplicates; [duplicate values, not points]\n",
    "            try splitting between each pair of consecutive values.\n",
    "            [We can radix sort the values in linear time, and if n is huge we should.]\n",
    "            \n",
    "            Clever Bit: As you scan sorted list from left to right, \n",
    "            you can update entropy in O(1) time per point! \n",
    "            [This is important for obtaining a fast tree-building time.] <--GO OVER THIS.\n",
    "            \n",
    "            Need to deal with case when a child is empty (skip that split)\n",
    "    \n",
    "        '''\n",
    "        #<= split value --> left, > split value --> right\n",
    "        \n",
    "        currNodeSize = len(train_data)\n",
    "        currClass1Count = sum(train_labels) \n",
    "        currClass0Count = currNodeSize - currClass1Count\n",
    "        currNodeEntropy = self.computeEntropy((currNodeSize, currClass0Count, currClass1Count))\n",
    "        bestInfoGain = -float(\"inf\")\n",
    " \n",
    "        labelDictRight = {0: currClass0Count, 1: currClass1Count}\n",
    "        labelDictLeft = {0:0, 1:0}\n",
    "        sizeRightSet = currNodeSize\n",
    "        sizeLeftSet = 0\n",
    "        for featureIndex in np.random.choice(train_data.shape[1], sizeFeatSubset, replace=False):\n",
    "            sortedIndices = np.argsort(train_data[:, featureIndex])\n",
    "            sortedVals = np.sort(train_data[:, featureIndex])\n",
    "            \n",
    "            lstUniqueVal, lstUniqueCounts = np.unique(sortedVals, return_counts=True)\n",
    "            runningIndex = - 1\n",
    "            for i in range(len(lstUniqueVal)):\n",
    "                runningIndex += (lstUniqueCounts[i])\n",
    "                tempSplitVal = lstUniqueVal[i]\n",
    "                \n",
    "                sizeLeftSet = runningIndex + 1\n",
    "                sizeRightSet = currNodeSize - sizeLeftSet\n",
    "                \n",
    "                labelDictLeft[1] = np.sum(train_labels[sortedIndices[:runningIndex+1]])\n",
    "                labelDictRight[1] = np.sum(train_labels[sortedIndices[runningIndex+1:]])\n",
    "                labelDictRight[0] = sizeRightSet - labelDictRight[1]\n",
    "                labelDictLeft[0] = sizeLeftSet - labelDictLeft[1]\n",
    "                \n",
    "                tupSizeLeftClassCounts = (sizeLeftSet, labelDictLeft[0], labelDictLeft[1])\n",
    "                tupSizeRightClassCounts = (sizeRightSet, labelDictRight[0], labelDictRight[1])\n",
    "                entropyAfterSplit = self.entropyAfterSplit(tupSizeLeftClassCounts, tupSizeRightClassCounts) \n",
    "                infoGain = currNodeEntropy - entropyAfterSplit\n",
    "                \n",
    "                if infoGain > bestInfoGain:\n",
    "                    bestInfoGain = infoGain\n",
    "                    bestSplitVal = tempSplitVal\n",
    "                    bestSplitIndex = featureIndex \n",
    "  \n",
    "        self.splitFeatIndex = bestSplitIndex \n",
    "        self.splitVal = bestSplitVal\n",
    "              \n",
    "    def entropyAfterSplit(self, tupSizeLeftClassCounts, tupSizeRightClassCounts):\n",
    "        '''\n",
    "            A method that takes in the result of a split: two histograms\n",
    "            that count the frequencies of labels on the ”left” and ”right” side of that split.\n",
    "            Calculates and outputs a scalar value representing the impurity (”badness”) of the \n",
    "            specified split on the input data. In lecture, this was refered to as H_after where H\n",
    "            is the function that computes entropy.\n",
    "            \n",
    "            Args:\n",
    "                tupSizeLeftClassCounts (tup) - \n",
    "                tupSizeRightClassCounts (tup) - \n",
    "        \n",
    "        '''\n",
    "        sizeLeftSet = tupSizeLeftClassCounts[0]\n",
    "        sizeRightSet = tupSizeRightClassCounts[0]\n",
    "\n",
    "        entropyLeft = self.computeEntropy(tupSizeLeftClassCounts)\n",
    "        entropyRight = self.computeEntropy(tupSizeRightClassCounts)\n",
    "        entropyAfterSplit = (entropyLeft * sizeLeftSet + entropyRight * sizeRightSet)/ (sizeLeftSet + sizeRightSet)\n",
    "        return entropyAfterSplit\n",
    "    \n",
    "    \n",
    "    def computeEntropy(self, tupleSizeSetClassCounts):\n",
    "        '''\n",
    "            Compute the entropy of a set (assume 2 classes). \n",
    "            \n",
    "            Args:\n",
    "                tupleSetSizeClassSize (tup) - (cardinality of set, cardinality of class 0 in set) \n",
    "        '''\n",
    "        sizeSet, countClass0, countClass1 = tupleSizeSetClassCounts\n",
    "        if (sizeSet==0):\n",
    "            return 0\n",
    "        probClass0 = countClass0/sizeSet\n",
    "        if (probClass0 == 1 or probClass0 == 0):\n",
    "            return 0\n",
    "        else: \n",
    "            entropy = (- probClass0 * np.log2(probClass0)) - ((1 - probClass0) * np.log2(1 - probClass0))\n",
    "        return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4a) Performance Eval - SPAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spamData = sp.io.loadmat(\"dist/spam_data.mat\")\n",
    "spamData.keys()\n",
    "spamTrainX = spamData['training_data']\n",
    "spamTrainY = spamData['training_labels']\n",
    "spamTest = spamData['test_data']\n",
    "\n",
    "combined = np.hstack([spamTrainX, spamTrainY.T])\n",
    "np.random.shuffle(combined)\n",
    "combined = imp().fit_transform(combined)\n",
    "\n",
    "spamTrainX = combined[:, :-1]\n",
    "spamTrainY = combined[:, -1]\n",
    "\n",
    "trainX, validX, trainY, validY = train_test_split(spamTrainX, spamTrainY, test_size=.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sizeFeatSubset = trainX.shape[1]\n",
    "spamDecision = DecisionTree(max_depth=50, leaf_condition=.84)\n",
    "spamDecision.train(trainX, trainY, sizeFeatSubset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validPredictionsSpam = spamDecision.predict(validX)\n",
    "validAccuracySpam = accuracy_score(validY, validPredictionsSpam)\n",
    "\n",
    "trainPredictionsSpam = spamDecision.predict(trainX)\n",
    "trainAccuracySpam = accuracy_score(trainY, trainPredictionsSpam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Performance on Spam\n",
      "----------------------------------\n",
      "validation accuracy: 76.63%\n",
      "training accuracy: 79.55%\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision Tree Performance on Spam\")\n",
    "print(\"----------------------------------\")\n",
    "print(\"validation accuracy: \" + str(round(validAccuracySpam * 100, 2)) + \"%\")\n",
    "print(\"training accuracy: \" + str(round(trainAccuracySpam * 100, 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testPredictionsSpam = spamDecision.predict(spamTest)\n",
    "# generateCsv(testPredictionsSpam, 'sundaySpam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Forest Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_trees = 50\n",
    "leaf_condition = 0.84\n",
    "sizeFeatSubset = int(np.sqrt(trainX.shape[1]))\n",
    "randForestSpam = RandomForest(num_trees, trainX, trainY, leaf_condition)\n",
    "randForestSpam.train(trainX, sizeFeatSubset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validPredictionsSpamForest = randForestSpam.predict(validX)\n",
    "validAccuracySpamForest = accuracy_score(validY, validPredictionsSpamForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainPredictionsSpamForest = randForestSpam.predict(trainX)\n",
    "trainAccuracySpamForest = accuracy_score(trainY, trainPredictionsSpamForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Forest Performance on Spam:\n",
      "----------------------------------\n",
      "validation accuracy 73.94%\n",
      "training accuracy 75.88%\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision Forest Performance on Spam:\")\n",
    "print(\"----------------------------------\")\n",
    "print(\"validation accuracy \" + str(round(validAccuracySpamForest * 100, 2)) + \"%\")\n",
    "print(\"training accuracy \" + str(round(trainAccuracySpamForest * 100, 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBMITTED DECISION TREE PREDICTIONS FOR SPAM\n",
      "----------------------------------\n",
      "KAGGLE NAME: maiarose\n",
      "KAGGLE SCORE: \n"
     ]
    }
   ],
   "source": [
    "print(\"SUBMITTED DECISION TREE PREDICTIONS FOR SPAM\")\n",
    "print(\"----------------------------------\")\n",
    "print(\"KAGGLE NAME: maiarose\")\n",
    "print(\"KAGGLE SCORE: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4b) Titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanicTest = pd.read_csv(\"hw5_titanic_dist/titanic_testing_data.csv\")\n",
    "titanicTrain = pd.read_csv(\"hw5_titanic_dist/titanic_training.csv\")\n",
    "\n",
    "titanicLabels = titanicTrain.iloc[0:, 0:1]\n",
    "titanicTrain = titanicTrain.loc[0:, [\"pclass\", \"sex\", \"age\", \"sibsp\", \"parch\", \"ticket\", \"fare\", \"cabin\", \"embarked\"]]\n",
    "\n",
    "headers = vec(np.array(list(titanicTrain))).T\n",
    "lstNumericFeat = arr(headers[:, [2, 5, 6]])\n",
    "lstCatFeat = arr(headers[:, [0, 1, 3, 4, 8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanicTrain['ticket'].replace(regex=True,inplace=True,to_replace=r'\\D',value=r'')\n",
    "titanicTest['ticket'].replace(regex=True,inplace=True,to_replace=r'\\D',value=r'')\n",
    "titanicTrain.drop('cabin', axis=1, inplace=True)\n",
    "titanicTest.drop('cabin', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexToElim = titanicTrain.index[titanicTrain.isnull().all(axis=1)][0]\n",
    "titanicLabels = titanicLabels.drop([705])\n",
    "\n",
    "lstDummies = [\"sex\", \"embarked\"]\n",
    "titanicTrain = preProcessData(titanicTrain, lstCatFeat, lstNumericFeat, lstDummies)\n",
    "titanicTest = preProcessData(titanicTest, lstCatFeat, lstNumericFeat, lstDummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstTitanicFeatures = list(titanicTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanicTrain = titanicTrain.as_matrix()\n",
    "titanicLabels = titanicLabels.as_matrix()\n",
    "titanicTest = titanicTest.as_matrix()\n",
    "\n",
    "combined = np.hstack([titanicTrain, titanicLabels])\n",
    "np.random.shuffle(combined)\n",
    "titanicTrainX = combined[:, :-1]\n",
    "titanicTrainY = combined[:, -1]\n",
    "\n",
    "trainX, validX, trainY, validY = train_test_split(titanicTrainX, titanicTrainY, test_size=.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Performance - Titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sizeFeatSubset = trainX.shape[1]\n",
    "titanicTree = DecisionTree(max_depth=50, leaf_condition=.84)\n",
    "titanicTree.train(trainX, trainY, sizeFeatSubset)\n",
    "validPredictionsTitanic = titanicTree.predict(validX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstCorrectLabelsValid = [1 for i in range(len(validY)) if validY[i]==validPredictionsTitanic[i]]\n",
    "validAccuracyTitanic = np.sum(lstCorrectLabelsValid)/len(validY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainPredictionsTitanic = titanicTree.predict(trainX)\n",
    "lstCorrectLabelsTrain = [1 for i in range(len(trainY)) if trainY[i]==trainPredictionsTitanic[i]]\n",
    "trainAccuracyTitanic = np.sum(lstCorrectLabelsTrain)/len(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Performance on Titanic\n",
      "----------------------------------\n",
      "validation accuracy: 80.0%\n",
      "training accuracy: 91.99%\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision Tree Performance on Titanic\")\n",
    "print(\"----------------------------------\")\n",
    "print(\"validation accuracy: \" + str(round(validAccuracyTitanic * 100, 2)) + \"%\")\n",
    "print(\"training accuracy: \" + str(round(trainAccuracyTitanic * 100, 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Forest Performance - Titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_trees = 50\n",
    "leaf_condition = 0.84\n",
    "sizeFeatSubset = int(np.sqrt(trainX.shape[1]))\n",
    "randForestTitanic = RandomForest(num_trees, trainX, trainY, leaf_condition)\n",
    "randForestTitanic.train(trainX, sizeFeatSubset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validPredictionsTitanicForest = randForestTitanic.predict(validX)\n",
    "lstCorrectLabelsForest = [1 for i in range(len(validY)) if validY[i]==validPredictionsTitanicForest[i]]\n",
    "validAccuracyTitanicForest = np.sum(lstCorrectLabelsForest)/len(validY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainPredictionsTitanicForest = randForestTitanic.predict(trainX)\n",
    "lstCorrectLabelsForest = [1 for i in range(len(trainY)) if trainY[i]==trainPredictionsTitanicForest[i]]\n",
    "trainAccuracyTitanicForest = np.sum(lstCorrectLabelsForest)/len(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBMITTED DECISION TREE PREDICTIONS FOR Titanic\n",
      "----------------------------------\n",
      "KAGGLE NAME: maiarose\n",
      "KAGGLE SCORE: 83.87%\n"
     ]
    }
   ],
   "source": [
    "print(\"SUBMITTED DECISION TREE PREDICTIONS FOR Titanic\")\n",
    "print(\"----------------------------------\")\n",
    "print(\"KAGGLE NAME: maiarose\")\n",
    "print(\"KAGGLE SCORE: 83.87%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testPredictionsTitanicTree = titanicTree.predict(titanicTest)\n",
    "generateCsv(testPredictionsTitanicTree, 'sundayTitanic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4c) Census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "censusTest = pd.read_csv(\"hw5_census_dist/test_data.csv\")\n",
    "censusTrain = pd.read_csv(\"hw5_census_dist/train_data.csv\")\n",
    "\n",
    "censusLabels = censusTrain.iloc[:, -1].to_frame()\n",
    "censusTrain = censusTrain.loc[:, [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\"]]\n",
    "\n",
    "headers = vec(np.array(list(censusTrain))).T\n",
    "lstNumericFeat = arr(headers[:, [0, 2, 4, 10, 11, 12]])\n",
    "lstCatFeat = arr(headers[:, [1, 3, 5, 6, 7, 8, 9, 13]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstDummies = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"]\n",
    "censusTrain = preProcessData(censusTrain, lstCatFeat, lstNumericFeat, lstDummies)\n",
    "censusTest = preProcessData(censusTest, lstCatFeat, lstNumericFeat, lstDummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstCensusFeatures = list(censusTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "censusTrain = censusTrain.as_matrix()\n",
    "censusLabels = censusLabels.as_matrix()\n",
    "censusTest = censusTest.as_matrix()\n",
    "\n",
    "combined = np.hstack([censusTrain, censusLabels])\n",
    "np.random.shuffle(combined)\n",
    "\n",
    "censusTrainX = combined[:, :-1]\n",
    "censusTrainY = combined[:, -1]\n",
    "\n",
    "trainX, validX, trainY, validY = train_test_split(censusTrainX, censusTrainY, test_size=.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Performance - Census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "censusTree = DecisionTree(max_depth=50, leaf_condition=.84)\n",
    "sizeFeatSubset = trainX.shape[1]\n",
    "censusTree.train(trainX, trainY, sizeFeatSubset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validPredictionsCensus = censusTree.predict(validX)\n",
    "validAccuracyCensus = accuracy_score(validY, validPredictionsCensus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainPredictionsCensus = censusTree.predict(trainX)\n",
    "trainAccuracyCensus = accuracy_score(trainY, trainPredictionsCensus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testPredictionsCensusTree = censusTree.predict(censusTest)\n",
    "generateCsv(testPredictionsCensusTree, 'sundayCensus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Performance on Census\n",
      "----------------------------------\n",
      "validation accuracy: 82.74%\n",
      "training accuracy: 93.57%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision Tree Performance on Census\")\n",
    "print(\"----------------------------------\")\n",
    "print(\"validation accuracy: \" + str(round(validAccuracyCensus * 100, 2)) + \"%\")\n",
    "print(\"training accuracy: \" + str(round(trainAccuracyCensus * 100, 2)) + \"%\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Forest Performance - Census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_trees = 10\n",
    "leaf_condition = 0.84\n",
    "sizeSubsetFeat = int(np.sqrt(trainX.shape[1]))\n",
    "randForestCensus = RandomForest(num_trees, trainX, trainY, leaf_condition)\n",
    "randForestCensus.train(trainX, sizeSubsetFeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validPredictionsCensusForest = randForestCensus.predict(validX)\n",
    "validAccuracyCensusForest = accuracy_score(validY, validPredictionsCensusForest)\n",
    "\n",
    "trainPredictionsCensusForest = randForestCensus.predict(trainX)\n",
    "trainAccuracyCensusForest = accuracy_score(trainY, trainPredictionsCensusForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Forest Performance on Census\n",
      "----------------------------------\n",
      "validation accuracy 79.0%\n",
      "training accuracy 83.98%\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision Forest Performance on Census\")\n",
    "print(\"----------------------------------\")\n",
    "print(\"validation accuracy \" + str(round(validAccuracyTitanicForest * 100, 2)) + \"%\")\n",
    "print(\"training accuracy \" + str(round(trainAccuracyTitanicForest * 100, 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did not use any extra features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5b) Spam Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def chooseRandSamples(trainX, trainY):\n",
    "    '''\n",
    "        Selects and returns a random point for each class (0 and 1) from the data set\n",
    "        \n",
    "        Args:\n",
    "            trainX (ndarray) - training data with n samples, d features \n",
    "            trainY (ndarray) - (nx1) array of labels for the training data\n",
    "    '''\n",
    "    sampleClass0 = None\n",
    "    sampleClass1 = None\n",
    "    while True:\n",
    "        random = np.random.choice(trainX.shape[0], replace=False)\n",
    "        if (trainY[random] == 1):\n",
    "            sampleClass1 = trainX[random, :]\n",
    "        else:\n",
    "            sampleClass0 = trainX[random, :]\n",
    "        if (sampleClass1 is not None and sampleClass0 is not None):\n",
    "            return sampleClass0, sampleClass1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampleClass0, sampleClass1 = chooseRandSamples(spamTrainX, spamTrainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mini_matrix = np.vstack([sampleClass0, sampleClass1])\n",
    "labels = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstFeaturesSpam = [\"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\", \"prescription\", \"creative\", \"height\", \"featured\", \"differ\", \"width\", \"other\", \"energy\", \"business\", \"message\", \"volumes\", \"revision\", \"path\", \"meter\", \"memo\", \"planning\", \"pleased\", \"record\", \"out\", \";\", \"$\", \"#\", \"!\", \"(\", \"[\", \"&\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPAM DECISION TREE ON TWO SAMPLES\n",
      "KEY: 0 --> Ham, 1--> Spam \n",
      "\n",
      "Predicting sample0: \n",
      "Actual class: 0\n",
      "('!') <= 0.0\n",
      "('(') <= 0.0\n",
      "('meter') <= 0.0\n",
      "(';') > 1.0\n",
      "('[') <= 0.0\n",
      "Therefore the class is 0\n",
      "\n",
      "Predicting sample1: \n",
      "Actual class: 1\n",
      "('!') <= 0.0\n",
      "('(') <= 0.0\n",
      "('meter') <= 0.0\n",
      "(';') <= 1.0\n",
      "('creative') <= 0.0\n",
      "('energy') <= 0.0\n",
      "('money') <= 0.0\n",
      "('pain') <= 0.0\n",
      "('&') <= 0.0\n",
      "('$') > 0.0\n",
      "('[') <= 0.0\n",
      "('message') <= 0.0\n",
      "('volumes') <= 0.0\n",
      "('prescription') <= 0.0\n",
      "('spam') <= 0.0\n",
      "('$') <= 42.0\n",
      "('business') <= 1.0\n",
      "('out') <= 0.0\n",
      "('planning') <= 0.0\n",
      "('record') <= 1.0\n",
      "('#') <= 3.0\n",
      "('pleased') <= 0.0\n",
      "('other') <= 0.0\n",
      "('path') <= 0.0\n",
      "('$') <= 18.0\n",
      "('$') <= 13.0\n",
      "('$') <= 9.0\n",
      "('private') <= 0.0\n",
      "('$') <= 5.0\n",
      "(';') <= 0.0\n",
      "('bank') <= 0.0\n",
      "('$') <= 1.0\n",
      "('business') <= 0.0\n",
      "('record') <= 0.0\n",
      "('#') <= 0.0\n",
      "Therefore the class is 0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"SPAM DECISION TREE ON TWO SAMPLES\")\n",
    "\n",
    "print(\"KEY: 0 --> Ham, 1--> Spam \\n\")\n",
    "spamDecision.predictWithPrint(mini_matrix, labels, lstFeaturesSpam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5c) Spam Forest Popular Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest: 50 trees\n",
      "Feature: 9 --> Val: 0.0 (5 trees)\n",
      "Feature: 21 --> Val: 0.0 (1 trees)\n",
      "Feature: 3 --> Val: 0.0 (4 trees)\n",
      "Feature: 26 --> Val: 0.0 (5 trees)\n",
      "Feature: 6 --> Val: 0.0 (2 trees)\n",
      "Feature: 29 --> Val: 0.0 (6 trees)\n",
      "Feature: 20 --> Val: 0.0 (1 trees)\n",
      "Feature: 16 --> Val: 0.0 (6 trees)\n",
      "Feature: 5 --> Val: 0.0 (3 trees)\n",
      "Feature: 28 --> Val: 0.0 (2 trees)\n",
      "Feature: 19 --> Val: 0.0 (5 trees)\n",
      "Feature: 25 --> Val: 3.0 (1 trees)\n",
      "Feature: 31 --> Val: 0.0 (5 trees)\n",
      "Feature: 13 --> Val: 0.0 (4 trees)\n"
     ]
    }
   ],
   "source": [
    "dictSplitFeatVal2Count = {(tree.root.splitFeatIndex, tree.root.splitVal):0 for tree in randForestSpam.trees}\n",
    "print(\"Forest: \" + str(len(randForestSpam.trees)) + \" trees\")\n",
    "for tree in randForestSpam.trees:\n",
    "    dictSplitFeatVal2Count[(tree.root.splitFeatIndex, tree.root.splitVal)] +=1\n",
    "for tup, count in dictSplitFeatVal2Count.items():\n",
    "    print(\"Feature: \" + str(tup[0]) + \" --> Val: \" + str(tup[1]) + \" (\" + str(count) + \" trees)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NA -- didn't change any features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "censusClass0, censusClass1 = chooseRandSamples(censusTrainX, censusTrainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mini_matrix_census = np.vstack([censusClass0, censusClass1])\n",
    "labels = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CENSUS DECISION TREE ON TWO SAMPLES\n",
      "KEY: 0 --> Under 50,000, 1--> Over 50,000 \n",
      "\n",
      "Predicting sample0: \n",
      "Actual class: 0\n",
      "('marital-status_Married-civ-spouse') <= 0\n",
      "Therefore the class is 0\n",
      "\n",
      "Predicting sample1: \n",
      "Actual class: 1\n",
      "('marital-status_Married-civ-spouse') > 0\n",
      "('education-num') > 11\n",
      "('capital-gain') <= 5013\n",
      "('capital-loss') <= 1740\n",
      "('age') > 28\n",
      "('hours-per-week') > 30\n",
      "('capital-gain') <= 4386\n",
      "('capital-loss') <= 1485\n",
      "('education-num') > 13\n",
      "('race_White') > 0\n",
      "('occupation_Exec-managerial') > 0\n",
      "Therefore the class is 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"CENSUS DECISION TREE ON TWO SAMPLES\")\n",
    "print(\"KEY: 0 --> Under 50,000, 1--> Over 50,000 \\n\")\n",
    "censusTree.predictWithPrint(mini_matrix_census, labels, lstCensusFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6c) Popular Forest Splits (Census)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest: 10 trees\n",
      "Feature: 5 --> Val: 41 (1 trees)\n",
      "Feature: 64 --> Val: 0 (2 trees)\n",
      "Feature: 2 --> Val: 12 (1 trees)\n",
      "Feature: 53 --> Val: 0 (2 trees)\n",
      "Feature: 33 --> Val: 0 (1 trees)\n",
      "Feature: 56 --> Val: 0 (1 trees)\n",
      "Feature: 54 --> Val: 0 (1 trees)\n",
      "Feature: 0 --> Val: 28 (1 trees)\n"
     ]
    }
   ],
   "source": [
    "dictSplitFeatVal2Count = {(tree.root.splitFeatIndex, tree.root.splitVal):0 for tree in randForestCensus.trees}\n",
    "print(\"Forest: \" + str(len(randForestCensus.trees)) + \" trees\")\n",
    "for tree in randForestCensus.trees:\n",
    "    dictSplitFeatVal2Count[(tree.root.splitFeatIndex, tree.root.splitVal)] +=1\n",
    "for tup, count in dictSplitFeatVal2Count.items():\n",
    "    print(\"Feature: \" + str(tup[0]) + \" --> Val: \" + str(tup[1]) + \" (\" + str(count) + \" trees)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6d) Experimenting with Depths (Census)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depths = list(range(1, 50, 10))\n",
    "trainX, validX, trainY, validY = train_test_split(censusTrainX, censusTrainY, test_size=.2, random_state=42)\n",
    "accuraciesCensus = []\n",
    "\n",
    "sizeFeatSubset = trainX.shape[1]\n",
    "for i in depths:\n",
    "    censusTree = DecisionTree(max_depth=i, leaf_condition=.84)\n",
    "    censusTree.train(trainX, trainY, sizeFeatSubset)\n",
    "\n",
    "    validPredictionsCensus = censusTree.predict(validX)\n",
    "    validAccuracyCensus = accuracy_score(validY, validPredictionsCensus)\n",
    "    accuraciesCensus.append(validAccuracyCensus)\n",
    "\n",
    "best_depth = depths[np.argmax(accuraciesCensus)]\n",
    "best_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEXCAYAAABcRGizAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f657c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotAccuracies(accuraciesCensus, depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Titanic Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanicShallowTree = DecisionTree(max_depth=3, leaf_condition=.84)\n",
    "trainX, validX, trainY, validY = train_test_split(titanicTrainX, titanicTrainY, test_size=.1, random_state=42)\n",
    "sizeFeatSubset = trainX.shape[1]\n",
    "titanicShallowTree.train(trainX, trainY, sizeFeatSubset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root: feat sex_female --> val 0\n",
      "level 1 left: feat ticket --> val 2668\n",
      "level 1 right: feat pclass --> val 2.0\n",
      "level 2 left>left feat age --> val 6.0\n",
      "level 2 left>right LEAF class 0\n",
      "level 2 right>left LEAF class 1\n",
      "level 2 right>right: feat fare --> val 23.25\n",
      "level 3 left>left>left LEAF class 1\n",
      "level 3 left>left>right: LEAF class 0\n",
      "level 3 right>right>right LEAF class 0\n",
      "level 3 right>right>left LEAF class 1\n"
     ]
    }
   ],
   "source": [
    "root = titanicShallowTree.root\n",
    "print('root: feat ' + str(lstTitanicFeatures[root.splitFeatIndex]) + \" --> val \" + str(root.splitVal))\n",
    "left = root.left\n",
    "print('level 1 left: feat ' + str(lstTitanicFeatures[left.splitFeatIndex]) + \" --> val \" + str(left.splitVal))\n",
    "right = root.right\n",
    "print('level 1 right: feat ' + str(lstTitanicFeatures[right.splitFeatIndex]) + \" --> val \" + str(right.splitVal))\n",
    "leftLeft = left.left\n",
    "leftRight = left.right\n",
    "rightLeft = right.left\n",
    "rightRight = right.right\n",
    "print('level 2 left>left feat ' + str(lstTitanicFeatures[leftLeft.splitFeatIndex]) + \" --> val \" + str(leftLeft.splitVal))\n",
    "print('level 2 left>right LEAF class ' + str(leftRight.label))\n",
    "\n",
    "print('level 2 right>left LEAF class ' + str(rightLeft.label))\n",
    "print('level 2 right>right: feat ' + str(lstTitanicFeatures[rightRight.splitFeatIndex]) + \" --> val \" + str(rightRight.splitVal))\n",
    "\n",
    "#leaves\n",
    "leftLeftLeft = leftLeft.left \n",
    "leftLeftRight = leftLeft.right \n",
    "rightRightLeft = rightRight.left\n",
    "rightRightRight = rightRight.right \n",
    "\n",
    "print('level 3 left>left>left LEAF class ' + str(leftLeftLeft.label))\n",
    "print('level 3 left>left>right: LEAF class ' + str(leftLeftRight.label))\n",
    "print('level 3 right>right>right LEAF class ' + str(rightRightRight.label))\n",
    "print('level 3 right>right>left LEAF class ' + str(rightRightLeft.label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [189hw]",
   "language": "python",
   "name": "Python [189hw]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
