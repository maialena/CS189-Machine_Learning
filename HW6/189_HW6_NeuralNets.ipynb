{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW6 Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Maia Rosengarten <br/>\n",
    "SID: 23572580 <br/>\n",
    "Login: cs-<br/>\n",
    "April 14, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy as sp\n",
    "from scipy import io\n",
    "from scipy.stats import logistic as sig\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import normalize\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NP FNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vec(arr): # d b 1 --> (d, 1)\n",
    "    return arr.reshape((arr.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def arr(vec): # 1 by d --> (d, )\n",
    "    return vec.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTIL FNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotAccuracies(accuracies, depths):\n",
    "    '''\n",
    "        Plots accuracies as a function od depth\n",
    "        Args:\n",
    "            costs (ndarray) - lst of costs per iteration of gradient descent\n",
    "    '''\n",
    "   \n",
    "    plt.plot(depths, accuracies)\n",
    "    plt.title(\"Prediction Evaluation Decision Tree On Census Data\")\n",
    "    plt.xlabel('max_depth')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotCosts(costs, iterations):\n",
    "    '''\n",
    "        Plots accuracies as a function od depth\n",
    "        Args:\n",
    "            costs (ndarray) - lst of costs per iteration of gradient descent\n",
    "    '''\n",
    "   \n",
    "    plt.plot(iterations, costs)\n",
    "    plt.title(\"Costs\")\n",
    "    plt.xlabel('costs')\n",
    "    plt.ylabel('numIter')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateCsv(aryPredictions, strCsvName):\n",
    "    '''\n",
    "    Writes predictions of testSet to csv file\n",
    "    Args:\n",
    "        aryPredictions (ndarray) - (nx1)-array of predictions given size n test (or valid) set\n",
    "        strCsvName (str) - name of csv file to write to\n",
    "    '''\n",
    "    with open(strCsvName + '.csv', 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for i in range(len(aryPredictions)):\n",
    "            writer.writerow([i, aryPredictions[i]])\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def computeCost(X, y, w, regConst):\n",
    "#     prob = sp.special.expit(np.dot(X, w))\n",
    "#     ret = 1/X.shape[0] * (regConst * np.linalg.norm(w)**2) - (vec(y).T.dot(np.log(prob + 0.000001)) + (1-vec(y)).T.dot(np.log(1 - prob + 0.000001)))\n",
    "#     return arr(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EnsembleNet:\n",
    "    def __init__(self):\n",
    "        self.nets = []\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, trainX, trainY, V=None, W=None, l2=None, numHiddenLayers=800, learnRateW=0.01, learnRateV=0.01, hasDecay=True, hasDropOut=False, momentum=None, batchSize=None, numIterations=10000):\n",
    "        self.X = trainX\n",
    "        self.Y = trainY\n",
    "        self.sizeH = numHiddenLayers\n",
    "        self.learnRateW = learnRateW\n",
    "        self.learnRateV = learnRateV\n",
    "        self.hasDecay = hasDecay\n",
    "        self.hasDropOut = hasDropOut\n",
    "        self.momentum = momentum\n",
    "        self.batchSize = batchSize\n",
    "        self.numIter = numIterations\n",
    "        self.V = V \n",
    "        self.W = W\n",
    "        self.predictions = []\n",
    "        self.l2 = l2\n",
    "        self.costs = []\n",
    "        if V==None and W==None:\n",
    "            self.initWeights()\n",
    "    \n",
    "    def setLearningRates(self, learnRateW, learnRateV):\n",
    "        self.learnRateW = learnRateW\n",
    "        self.learnRateV = learnRateV\n",
    "    \n",
    "    def setWeights(self, W, V):\n",
    "        self.V = V\n",
    "        self.W = W\n",
    "    \n",
    "    def setHasDecay(self, hasDecay):\n",
    "        self.hasDecay = hasDecay\n",
    "    \n",
    "    def setMomentum(self, momentum):\n",
    "        self.momentum = momentum\n",
    "    \n",
    "    def initWeights(self):\n",
    "        numFeatures = self.X.shape[1]\n",
    "#         self.V = np.random.normal(loc=0.0, scale=1/np.sqrt(numFeatures), size=(self.sizeH, numFeatures))\n",
    "#         self.W = np.random.normal(loc=0.0, scale=1/np.sqrt(self.sizeH + 1), size=(26, self.sizeH + 1))\n",
    "        self.V = np.random.normal(loc=0.0, scale=0.01, size=(self.sizeH, numFeatures))\n",
    "        self.W = np.random.normal(loc=0.0, scale=0.01, size=(26, self.sizeH + 1))\n",
    "    \n",
    "    def trainSGD(self):\n",
    "        epoch = self.X.shape[0]\n",
    "        numFeatures = self.X.shape[1]\n",
    "        for i in range(self.numIter):   \n",
    "            index = i%epoch\n",
    "            if (index==0 and i!=0):\n",
    "                self.X, self.Y = shuffle(self.X, self.Y, random_state=42)\n",
    "            \n",
    "            sample = vec(self.X[index])\n",
    "            y = vec(self.Y[index])\n",
    "            \n",
    "            if self.hasDropOut and i%10000==0:\n",
    "                inputDropOutIndices = np.random.choice(range(0, numFeatures), size=math.ceil(0.10*numFeatures), replace=False)\n",
    "                hiddenDropOutIndices = np.random.choice(range(0, self.sizeH), size=math.ceil(0.5*self.sizeH), replace=False)\n",
    "                sample[inputDropOutIndices]=0 \n",
    "                h = np.tanh(np.dot(self.V.T, sample)) \n",
    "                h[hiddenDropOutIndices]=0\n",
    "            else:\n",
    "                h = np.tanh(np.dot(self.V, sample)) \n",
    "            h = np.vstack((h, np.array(1)))\n",
    "            z = sp.special.expit(np.dot(self.W, vec(h)))\n",
    "\n",
    "            grad_w = grad = np.dot(z-y, h.T)\n",
    "            \n",
    "            diag = np.diag(arr(np.dot((z-y).T, W)))\n",
    "            dLdH = 1-np.square(arr(h))\n",
    "            grad_v = np.dot(np.dot(diag.T, vec(dLdH)), sample.T)[:-1]\n",
    "            \n",
    "            if self.hasDecay and i%50000==0:\n",
    "                self.learnRateW = 0.001\n",
    "                self.learnRateV = 0.001\n",
    "            \n",
    "            self.W = self.W - self.learnRateW * grad_w\n",
    "            self.V = self.V - self.learnRateV * grad_v\n",
    "            self.costs.append(computeCrossEntropyLoss(z, y))\n",
    "        return self.V, self.W\n",
    "\n",
    "\n",
    "    def trainMiniBatch(self):\n",
    "        epoch = self.X.shape[0]\n",
    "        t = 0\n",
    "        \n",
    "        while t < self.numIter:\n",
    "            if (t%20000==0):\n",
    "                print('iter ' + str(t))\n",
    "            \n",
    "            i = t % epoch\n",
    "            j = (t + self.batchSize) % epoch\n",
    "\n",
    "            if (t==epoch):\n",
    "                self.X, self.Y = shuffle(self.X, self.Y, random_state=42)\n",
    "                i = 0\n",
    "                j = self.batchSize\n",
    "            else:\n",
    "                i = t % epoch\n",
    "                j = (t + self.batchSize) % epoch \n",
    "            \n",
    "            samples = self.X[i:j]\n",
    "            y = self.Y[i:j]\n",
    "  \n",
    "            if self.hasDropOut:\n",
    "                numFeatures = self.X.shape[1]\n",
    "                inputDropOutIndices = np.random.choice(range(numFeatures), size=math.ceil(0.10*numFeatures), replace=False)\n",
    "                hiddenDropOutIndices = np.random.choice(range(self.sizeH), size=math.ceil(0.5*self.sizeH), replace=False) \n",
    "                samples.T[inputDropOutIndices]=0\n",
    "                h = np.tanh(np.dot(self.V, samples.T)) \n",
    "                h[hiddenDropOutIndices]=0\n",
    "            else:\n",
    "                h = np.tanh(np.dot(self.V, samples.T)) \n",
    "            \n",
    "            h = np.vstack((h, np.array([1]*h.shape[1])))\n",
    "            z = sp.special.expit(np.dot(self.W, h))\n",
    "         \n",
    "        \n",
    "            zMinusY = (z-y.T)   \n",
    "            if (self.l2):\n",
    "                reg = 2*self.l2\n",
    "                grad_w = np.dot(zMinusY, h.T) + reg*self.W\n",
    "                grad_v = np.multiply(((zMinusY.T).dot(self.W)).T, 1-h**2).dot(samples)[:-1] + reg*self.V\n",
    "            else:\n",
    "                grad_w = zMinusY.dot(h.T)\n",
    "                grad_v = np.multiply(((zMinusY.T).dot(self.W)).T, 1-h**2).dot(samples)[:-1]\n",
    "                \n",
    "            if self.hasDecay and t%50000==0 and t!=0:\n",
    "                self.learnRateW = 0.001\n",
    "                self.learnRateV = 0.001\n",
    "                \n",
    "            self.W = self.W - (self.learnRateW * grad_w)\n",
    "            self.V = self.V - (self.learnRateV * grad_v)\n",
    "            self.costs.append(self.computeCrossEntropyLossBatch(z, y))\n",
    "            \n",
    "            t+=self.batchSize\n",
    "            \n",
    "        return self.V, self.W\n",
    "    \n",
    "\n",
    "    \n",
    "#     def computeGradVBatch(self, indices, y, z, h):\n",
    "#         '''\n",
    "#             BATCH\n",
    "#                 samples: (50, 785)\n",
    "#                 y: (50,26)\n",
    "#                 z: (26,50)\n",
    "#                 h: (801,50)\n",
    "#                 W: (26, 801)\n",
    "#                 V: (800, 785)\n",
    "#                 dHdL: (50,801)\n",
    "                \n",
    "#                 ret grad (800x785)\n",
    "#         '''\n",
    "#         samples = self.X[indices]\n",
    "#         dLdH = np.dot((z.T-y), self.W)\n",
    "#         prod = np.multiply(dLdH.T, 1-np.square(h))\n",
    "#         grad = np.dot(prod, samples)[:-1]\n",
    "#         return grad\n",
    "    \n",
    "    \n",
    "    def plotCosts(self, costs, iterations, strTypeGradDescent=\"MiniBatch\"):\n",
    "        '''\n",
    "            Plots accuracies as a function od depth\n",
    "            Args:\n",
    "                costs (ndarray) - lst of costs per iteration of gradient descent\n",
    "        '''\n",
    "\n",
    "        plt.plot(iterations, costs)\n",
    "        plt.title(\"Costs \" + strTypeGradDescent)\n",
    "        plt.xlabel('costs')\n",
    "        plt.ylabel('numIter')\n",
    "        plt.show()\n",
    "    \n",
    "    def computeCrossEntropyLossBatch(self, z, y):\n",
    "        '''\n",
    "            y: (50x26)\n",
    "            z: (26x50)\n",
    "            ret: (50x1)\n",
    "        \n",
    "        '''\n",
    "        cost = np.sum((-y.dot(np.log(z)) + (1 - y).dot(np.log(1-z)))) * 1/(self.batchSize)\n",
    "        return cost\n",
    "    \n",
    "    def predict(self, testX):\n",
    "        sizeData = testX.shape[0]\n",
    "        for i in range(sizeData):\n",
    "            if (i%50000==0):\n",
    "                print('iter ' + str(i))\n",
    "            h = np.tanh(np.dot(self.V, vec(self.X[i].T)))\n",
    "            h = np.vstack((h, np.array(1)))\n",
    "            z = sp.special.expit(np.dot(self.W, h))\n",
    "            prediction = np.argmax(z)\n",
    "            self.predictions.append(prediction+1)\n",
    "        return self.predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictLetters = sp.io.loadmat(\"hw6_data_dist/letters_data.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maialena/anaconda/envs/189hw/lib/python3.5/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "train_x = dictLetters['train_x']\n",
    "train_y = dictLetters['train_y']\n",
    "test_x = dictLetters['test_x']\n",
    "\n",
    "# combined = np.hstack((train_x, train_y))\n",
    "# np.random.shuffle(combined)\n",
    "# train_x = combined[:, :-1]\n",
    "# train_y = combined[:, -1]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# normalizer = scaler.fit(train_x)\n",
    "# train_x = normalizer.transform(train_x)\n",
    "# test_x = normalizer.transform(test_x)\n",
    "train_x = scaler.fit_transform(train_x)\n",
    "train_x, train_y = shuffle(train_x, train_y, random_state=42)\n",
    "train_x = np.hstack((train_x, np.ones(shape=(train_x.shape[0], 1))))\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=.2, random_state=42)\n",
    "# train_x = train_x[:200]\n",
    "# train_y = train_y[:200]\n",
    "one_hot_train_y = pd.get_dummies(arr(train_y)).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = NeuralNet(train_x, one_hot_train_y, numHiddenLayers=800, batchSize=50, numIterations=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maialena/anaconda/envs/189hw/lib/python3.5/site-packages/ipykernel/__main__.py:179: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "V, W = net.trainMiniBatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0\n",
      "iter 50000\n"
     ]
    }
   ],
   "source": [
    "predictions = net.predict(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61064703525641029"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(predictions, train_y)\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEWCAYAAACqitpwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c1226a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net.plotCosts(net.costs, range(math.ceil(net.numIter/net.batchSize)), \"miniBatch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train acc 78% with batch 100, numiter 100,000, l2=0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#numIterations = 10000, decayRate is every ceil(epoch/k) *=0.8 --> 79%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "    \n",
    "    \n",
    "#     def computeHiddenValues(self, indexRanSample, inputDropOutIndices=None, hiddenDropOutIndices=None):\n",
    "#         '''\n",
    "#             BATCH:\n",
    "#                 sample: 50x785\n",
    "#                 V: (800, 785)\n",
    "                \n",
    "#                 ret hidden: (801, 50)\n",
    "#             SGD:\n",
    "#                 sample: 1x785\n",
    "#                 V: (800, 785)\n",
    "                \n",
    "#                 ret hidden: (800, 1)\n",
    "#         '''\n",
    "#         sample = self.X[indexRanSample]\n",
    "#         hidden = np.tanh(np.dot(self.V, sample.T))  \n",
    "#         if self.hasDropOut:\n",
    "#             sample[inputDropOutIndices]=0 \n",
    "#             hidden[hiddenDropOutIndices]=0\n",
    "#         if (self.batchSize):\n",
    "#             hidden = np.vstack((hidden, np.array([1]*hidden.shape[1])))\n",
    "#         else:    \n",
    "#             hidden = np.vstack((vec(hidden), np.array(1)))\n",
    "#         return hidden\n",
    "\n",
    "#     def computeOutputValues(self, vectHidden):\n",
    "#         '''\n",
    "#             BATCH\n",
    "#                 self.W: (26x801)\n",
    "#                 vectHidden: (801x50)         \n",
    "#                 ret output: (26x50)\n",
    "#             SGD\n",
    "#                 self.W: (26x801)\n",
    "#                 vectHidden: (801x1)            \n",
    "#                 ret output: (26x1)\n",
    "                \n",
    "#         '''\n",
    "#         if (self.batchSize):\n",
    "#             output = sp.special.expit(np.dot(self.W, vectHidden))\n",
    "#         else:\n",
    "#             output = sp.special.expit(np.dot(self.W, vec(vectHidden)))\n",
    "#         return output\n",
    "\n",
    "\n",
    "#     def computeGradW(self, z, y, h):\n",
    "#         '''\n",
    "#             BATCH\n",
    "#                 z: (26,50)\n",
    "#                 y: (50,26)\n",
    "#                 h: (801x50)  \n",
    "#                 ret (26x801)\n",
    "            \n",
    "#             SGD\n",
    "#                 z = (26x1)\n",
    "#                 y = (26x1)\n",
    "#                 h = (801x1)\n",
    "#                 ret (26x801)\n",
    "#         '''\n",
    "#         if (self.batchSize):\n",
    "#             grad = np.dot(z-y.T, h.T)\n",
    "#         else:\n",
    "#             grad = np.dot(z-vec(y), h.T)\n",
    "#         return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [189hw]",
   "language": "python",
   "name": "Python [189hw]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
