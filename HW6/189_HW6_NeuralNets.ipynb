{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW6 Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Maia Rosengarten <br/>\n",
    "SID: 23572580 <br/>\n",
    "Login: cs-<br/>\n",
    "April 14, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy as sp\n",
    "from scipy import io\n",
    "from scipy.stats import logistic as sig\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import normalize\n",
    "# from sklearn.feature_extraction import DictVectorizer as dv\n",
    "\n",
    "# from sklearn.preprocessing import Imputer as imp\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NP FNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vec(arr): # d b 1 --> (d, 1)\n",
    "    return arr.reshape((arr.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def arr(vec): # 1 by d --> (d, )\n",
    "    return vec.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTIL FNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotAccuracies(accuracies, depths):\n",
    "    '''\n",
    "        Plots accuracies as a function od depth\n",
    "        Args:\n",
    "            costs (ndarray) - lst of costs per iteration of gradient descent\n",
    "    '''\n",
    "   \n",
    "    plt.plot(depths, accuracies)\n",
    "    plt.title(\"Prediction Evaluation Decision Tree On Census Data\")\n",
    "    plt.xlabel('max_depth')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateCsv(aryPredictions, strCsvName):\n",
    "    '''\n",
    "    Writes predictions of testSet to csv file\n",
    "    Args:\n",
    "        aryPredictions (ndarray) - (nx1)-array of predictions given size n test (or valid) set\n",
    "        strCsvName (str) - name of csv file to write to\n",
    "    '''\n",
    "    with open(strCsvName + '.csv', 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for i in range(len(aryPredictions)):\n",
    "            writer.writerow([i, aryPredictions[i]])\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#changing learning rate\n",
    "# def trainNeuralNetwork(X, Y, numIterations, learnRateW=0.01, learnRateV=0.01, decayRate=None):\n",
    "#     '''\n",
    "#         X: training images (images)\n",
    "#         y: training labels (labels)\n",
    "#         params: hyperparameters, e.g., learning rate ε, weight decay rate λ for L2 regularization, etc.\n",
    "#         1. Initialize the weights V and W randomly\n",
    "#         2. while (some stopping criterion)\n",
    "#             a. pick one image/label pair (Xi, yi) at random from the training set\n",
    "#             b. perform forward pass (compute hidden & output values and predicted labels) \n",
    "#             c. perform backward pass (compute partial derivatives needed for gradient descent) \n",
    "#             d. perform stochastic gradient descent update\n",
    "#         3. store V,W  \n",
    "#     '''\n",
    "    \n",
    "#     V = np.random.normal(loc=0.0, scale=1/np.sqrt(785), size=(800, 785))\n",
    "#     W = np.random.normal(loc=0.0, scale=1/np.sqrt(801), size=(26, 801))\n",
    "#     for i in range(numIterations):\n",
    "#         if (i > numIterations/2):\n",
    "#             learnRateW = 0.001\n",
    "#             learnRateV = 0.001\n",
    "#         randNum = np.random.choice(X.shape[0])\n",
    "#         sample = X[randNum]\n",
    "#         vectTrueLabel = Y[randNum]\n",
    "#         #forward_pass\n",
    "#         vectHidden = computeHiddenValues(X, V, randNum)\n",
    "#         vectPredicted = computeOutputValues(vectHidden, W)\n",
    "#         #backward_pass\n",
    "#         grad_w = computeGradW(vectPredicted, vectTrueLabel, vectHidden)\n",
    "#         grad_v = computeGradV(sample, vectTrueLabel, vectPredicted, vectHidden, W, V)\n",
    "#         W = W - learnRateW * grad_w\n",
    "#         V = V - learnRateV * grad_v\n",
    "#     return V, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preProcessData(data, meanMatrix, normalizingConstant):\n",
    "    '''\n",
    "    Center and normalize all your features. You are encouraged to try other preprocessing methods. \n",
    "    Shuffle the data!\n",
    "    The mean vector that you subtract from the training data must be the same as the vector you \n",
    "    subtract from the test data! \n",
    "    This also applies to the values you divide by to normalize.     \n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def computeCost(X, y, w, regConst):\n",
    "#     prob = sp.special.expit(np.dot(X, w))\n",
    "#     ret = 1/X.shape[0] * (regConst * np.linalg.norm(w)**2) - (vec(y).T.dot(np.log(prob + 0.000001)) + (1-vec(y)).T.dot(np.log(1 - prob + 0.000001)))\n",
    "#     return arr(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveCheckPoint():\n",
    "    '''\n",
    "    Write your code so that it saves its progress in a file when you terminate the program \n",
    "    (see the Python signal package for how to capture an interrupt signal) \n",
    "    and/or after every fixed number of iterations, and to write code that loads this file \n",
    "    and allows you to resume training. \n",
    "    \n",
    "    You might find it useful to look into the Python pickle module, or numpy.save. \n",
    "    This allows you to save and load arbitrary Python or numpy objects \n",
    "    (such as your neural network weights) as files.\n",
    "    \n",
    "    \n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNeuralNetworkDropout(X, Y, numIterations, learnRateW=0.01, learnRateV=0.01, decayRate=False):\n",
    "    '''\n",
    "        X: training images (images)\n",
    "        y: training labels (labels)\n",
    "        params: hyperparameters, e.g., learning rate ε, weight decay rate λ for L2 regularization, etc.\n",
    "        1. Initialize the weights V and W randomly\n",
    "        2. while (some stopping criterion)\n",
    "            a. pick one image/label pair (Xi, yi) at random from the training set\n",
    "            b. perform forward pass (compute hidden & output values and predicted labels) \n",
    "            c. perform backward pass (compute partial derivatives needed for gradient descent) \n",
    "            d. perform stochastic gradient descent update\n",
    "        3. store V,W  \n",
    "    '''\n",
    "    \n",
    "    V = np.random.normal(loc=0.0, scale=1/np.sqrt(785), size=(800, 785))\n",
    "    W = np.random.normal(loc=0.0, scale=1/np.sqrt(801), size=(26, 801))\n",
    "    epoch = X.shape[0]\n",
    "    numFeatures = X.shape[1]\n",
    "    sizeHidden = 800\n",
    "#     inputDropOutIndices = np.random.choice(range(0, numFeatures), size=math.ceil(0.10*numFeatures), replace=False)\n",
    "#     hiddenDropOutIndices = np.random.choice(range(0,sizeHidden), size=math.ceil(0.5*sizeHidden), replace=False)\n",
    "    \n",
    "    for i in range(numIterations):\n",
    "        randNum = np.random.choice(epoch)\n",
    "        sample = X[randNum]\n",
    "        vectTrueLabel = Y[randNum]\n",
    "        vectHidden = computeHiddenValues(X, V, randNum)\n",
    "        \n",
    "        if i %100000 == 0:\n",
    "            inputDropOutIndices = np.random.choice(range(0, numFeatures), size=math.ceil(0.10*numFeatures), replace=False)\n",
    "            hiddenDropOutIndices = np.random.choice(range(0,sizeHidden), size=math.ceil(0.5*sizeHidden), replace=False)\n",
    "            vectHidden = computeHiddenValues(X, V, randNum, inputDropOutIndices, hiddenDropOutIndices, True)\n",
    "\n",
    "        vectPredicted = computeOutputValues(vectHidden, W)\n",
    "        grad_w = computeGradW(vectPredicted, vectTrueLabel, vectHidden)\n",
    "        grad_v = computeGradV(sample, vectTrueLabel, vectPredicted, vectHidden, W, V)\n",
    "        if decayRate and i%10000==0:\n",
    "            learnRateW *= 0.5\n",
    "        W = W - learnRateW * grad_w\n",
    "        V = V - learnRateV * grad_v\n",
    "    return V, W\n",
    "\n",
    "\n",
    "def trainNeuralNetwork(X, Y, numIterations, learnRateW=0.01, learnRateV=0.01, decayRate=True):\n",
    "    '''\n",
    "        X: training images (images)\n",
    "        y: training labels (labels)\n",
    "        params: hyperparameters, e.g., learning rate ε, weight decay rate λ for L2 regularization, etc.\n",
    "        1. Initialize the weights V and W randomly\n",
    "        2. while (some stopping criterion)\n",
    "            a. pick one image/label pair (Xi, yi) at random from the training set\n",
    "            b. perform forward pass (compute hidden & output values and predicted labels) \n",
    "            c. perform backward pass (compute partial derivatives needed for gradient descent) \n",
    "            d. perform stochastic gradient descent update\n",
    "        3. store V,W  \n",
    "    '''\n",
    "    \n",
    "    V = np.random.normal(loc=0.0, scale=1/np.sqrt(785), size=(800, 785))\n",
    "    W = np.random.normal(loc=0.0, scale=1/np.sqrt(801), size=(26, 801))\n",
    "    epoch = X.shape[0]\n",
    "    for i in range(numIterations):\n",
    "        randNum = np.random.choice(epoch)\n",
    "        sample = X[randNum]\n",
    "        vectTrueLabel = Y[randNum]\n",
    "\n",
    "        vectHidden = computeHiddenValues(X, V, randNum)\n",
    "        vectPredicted = computeOutputValues(vectHidden, W)\n",
    "\n",
    "        grad_w = computeGradW(vectPredicted, vectTrueLabel, vectHidden)\n",
    "        grad_v = computeGradV(sample, vectTrueLabel, vectPredicted, vectHidden, W, V)\n",
    "        if decayRate and i%50000==0:\n",
    "            learnRateW = 0.001\n",
    "        W = W - learnRateW * grad_w\n",
    "        V = V - learnRateV * grad_v\n",
    "    return V, W\n",
    "\n",
    "def computeHiddenValues(X, V, subset, inputDropOutIndices=None, hiddenDropOutIndices=None, hasDropOut=False):\n",
    "    sample = X[subset]\n",
    "    hidden = np.tanh(np.dot(V, sample.T))\n",
    "    if hasDropOut:\n",
    "        sample[inputDropOutIndices]=0 \n",
    "        hidden[hiddenDropOutIndices]=0\n",
    "    hidden = np.vstack((vec(hidden), np.array(1)))\n",
    "    return hidden\n",
    "    \n",
    "def computeOutputValues(vectHidden, W):\n",
    "    output = sp.special.expit(np.dot(W, vec(vectHidden)))\n",
    "    return output\n",
    "\n",
    "\n",
    "def computeGradW(z, y, h):\n",
    "    '''\n",
    "        y - true labels for point x (26,)\n",
    "        z - predicted labels for point x (26,1)\n",
    "        h = hidden layer vector (201,1)\n",
    "    '''\n",
    "    grad = np.dot(z-vec(y), h.T)\n",
    "    return grad\n",
    "\n",
    "def computeGradV(x, y, z, h, W, V):\n",
    "    '''\n",
    "        Args\n",
    "            x - sample point (785,)\n",
    "            y - true labels for point x (26,)\n",
    "            z - predicted labels for point x (26,1)\n",
    "            h = hidden layer vector (201,1)\n",
    "            W - weight matrix to output layer\n",
    "            V - weight matrix to hidden layer\n",
    "    \n",
    "    '''\n",
    "    #diagaonal expects a 0-diemnsional thing.\n",
    "    diag = np.diag(arr(np.dot((z-vec(y)).T, W)))\n",
    "    dLdH = 1-np.square(h)\n",
    "    grad = np.dot(np.dot(diag.T, dLdH), vec(x).T)[:-1]\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def miniBatchGradDescent(X, Y, numIterations=10000, learnRateW=0.01, learnRateV=0.01, decayRate=False, k=50):\n",
    "    '''\n",
    "    Sample k data points instead of one data point and average the gradient update over those data points. \n",
    "    If k = n this becomes batch gradient descent. \n",
    "    \n",
    "    Typically k is some number between 16 and 256 (50 is a good starting point). \n",
    "    Note that larger batches use more memory.\n",
    "    '''\n",
    "    V = np.random.normal(loc=0.0, scale=1/np.sqrt(785), size=(800, 785))\n",
    "    W = np.random.normal(loc=0.0, scale=1/np.sqrt(801), size=(26, 801))\n",
    "    epoch = X.shape[0]\n",
    "    for i in range(numIterations):\n",
    "#         indices = np.random.randint(0, epoch, size=k)\n",
    "#         samples = X[indices]\n",
    "#         vectTrueLabel = Y[indices]\n",
    "        #forward_pass\n",
    "        vectHidden = computeHiddenValuesBatch(X, V, indices) \n",
    "        vectPredicted = computeOutputValuesBatch(vectHidden, W) \n",
    "        #backward_pass\n",
    "        grad_w = computeGradWBatch(vectPredicted, vectTrueLabel, vectHidden)\n",
    "        grad_v = computeGradVBatch(samples, vectTrueLabel, vectPredicted, vectHidden, W, V)\n",
    " \n",
    "        gradientWTensor = np.apply_along_axes(computeGradWBatch(vectPredicted, vectTrueLabel, vectHidden), )\n",
    "        gradVTensor = np.apply_along_axes\n",
    "        \n",
    "        if decayRate and i%(math.ceil(epoch/k))==0:\n",
    "            learnRateW *= 0.8\n",
    "        W = W - learnRateW * grad_w\n",
    "        V = V - learnRateV * grad_v\n",
    "    return V, W\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def computeHiddenValuesBatch(X, V, subset):\n",
    "    sample = X[subset]\n",
    "    hidden = np.tanh(np.dot(V, sample.T))\n",
    "    hidden = np.vstack((hidden, np.array([1]*hidden.shape[1])))\n",
    "    return hidden\n",
    "\n",
    "def computeOutputValuesBatch(vectHidden, W):\n",
    "    output = sp.special.expit(np.dot(W, vectHidden))\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictNeuralNetwork(test_X, V, W):\n",
    "    '''\n",
    "    test_X: test images\n",
    "    V, W: network weights (previously trained)\n",
    "    \n",
    "    1. for each test image x\n",
    "        with V & W, perform forward pass (compute hidden & output values and predicted labels)\n",
    "    2. return all the predicted labels\n",
    "    \n",
    "    '''\n",
    "    predictions = []\n",
    "    for i in range(test_X.shape[0]):\n",
    "        vectHidden = computeHiddenValues(test_X, V, i)\n",
    "        vectPredict = computeOutputValues(vectHidden, W)\n",
    "        prediction = np.argmax(vectPredict)\n",
    "        predictions.append(prediction + 1)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictLetters = sp.io.loadmat(\"hw6_data_dist/letters_data.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maialena/anaconda/envs/189hw/lib/python3.5/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "train_x = dictLetters['train_x']\n",
    "train_y = dictLetters['train_y']\n",
    "test_x = dictLetters['test_x']\n",
    "\n",
    "\n",
    "# DO WE NORMALIZE BEFORE WE ADD THE BIAS OR AFTER?\n",
    "scaler = StandardScaler()\n",
    "normalizer = scaler.fit(train_x)\n",
    "train_x = normalizer.transform(train_x)\n",
    "test_x = normalizer.transform(test_x)\n",
    "\n",
    "combined = np.hstack((train_x, train_y))\n",
    "np.random.shuffle(combined)\n",
    "train_x = combined[:, :-1]\n",
    "train_y = combined[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x = np.hstack((train_x, np.ones(shape=(train_x.shape[0], 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_hot_train_y = pd.get_dummies(arr(train_y)).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numIterations=100000\n",
    "V, W = trainNeuralNetwork(train_x, one_hot_train_y, numIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = predictNeuralNetwork(valid_x, V, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(valid_y, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# V, W = miniBatchGradDescent(train_x, one_hot_train_y, numIterations=10000, decayRate=True, k=50)\n",
    "# predictions = predictNeuralNetwork(valid_x, V, W)\n",
    "# accuracy = accuracy_score(valid_y, predictions)\n",
    "# accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = train_x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#numIterations = 10000, decayRate is every ceil(epoch/k) *=0.8 --> 79%"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [189hw]",
   "language": "python",
   "name": "Python [189hw]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
